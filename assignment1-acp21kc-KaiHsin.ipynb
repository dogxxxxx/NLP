{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [COM6513] Assignment 1: Sentiment Analysis with Logistic Regression\n",
    "\n",
    "### Instructor: Nikos Aletras\n",
    "\n",
    "\n",
    "The goal of this assignment is to develop and test a **text classification** system for **sentiment analysis**, in particular to predict the sentiment of movie reviews, i.e. positive or negative (binary classification).\n",
    "\n",
    "\n",
    "\n",
    "For that purpose, you will implement:\n",
    "\n",
    "\n",
    "- Text processing methods for extracting Bag-Of-Word features, using \n",
    "    - n-grams (BOW), i.e. unigrams, bigrams and trigrams to obtain vector representations of documents where n=1,2,3 respectively. Two vector weighting schemes should be tested: (1) raw frequencies (**1 mark**); (2) tf.idf (**1 mark**). \n",
    "    - character n-grams (BOCN). A character n-gram is a contiguous sequence of characters given a word, e.g. for n=2, 'coffee' is split into {'co', 'of', 'ff', 'fe', 'ee'}. Two vector weighting schemes should be tested: (1) raw frequencies (**1 mark**); (2) tf.idf (**1 mark**). **Tip: Note the large vocabulary size!** \n",
    "    - a combination of the two vector spaces (n-grams and character n-grams) choosing your best performing wighting respectively (i.e. raw or tfidf). (**1 mark**) **Tip: you should merge the two representations**\n",
    "\n",
    "\n",
    "\n",
    "- Binary Logistic Regression (LR) classifiers that will be able to accurately classify movie reviews trained with: \n",
    "    - (1) BOW-count (raw frequencies) \n",
    "    - (2) BOW-tfidf (tf.idf weighted)\n",
    "    - (3) BOCN-count\n",
    "    - (4) BOCN-tfidf\n",
    "    - (5) BOW+BOCN (best performing weighting; raw or tfidf)\n",
    "\n",
    "\n",
    "\n",
    "- The Stochastic Gradient Descent (SGD) algorithm to estimate the parameters of your Logistic Regression models. Your SGD algorithm should:\n",
    "    - Minimise the Binary Cross-entropy loss function (**1 mark**)\n",
    "    - Use L2 regularisation (**1 mark**)\n",
    "    - Perform multiple passes (epochs) over the training data (**1 mark**)\n",
    "    - Randomise the order of training data after each pass (**1 mark**)\n",
    "    - Stop training if the difference between the current and previous development loss is smaller than a threshold (**1 mark**)\n",
    "    - After each epoch print the training and development loss (**1 mark**)\n",
    "\n",
    "\n",
    "\n",
    "- Discuss how did you choose hyperparameters (e.g. learning rate and regularisation strength) for each LR model? You should use a table showing model performance using different set of hyperparameter values. (**2 marks). **Tip: Instead of using all possible combinations, you could perform a random sampling of combinations.**\n",
    "\n",
    "\n",
    "- After training each LR model, plot the learning process (i.e. training and validation loss in each epoch) using a line plot. Does your model underfit, overfit or is it about right? Explain why. (**1 mark**). \n",
    "\n",
    "\n",
    "- Identify and show the most important features (model interpretability) for each class (i.e. top-10 most positive and top-10 negative weights). Give the top 10 for each class and comment on whether they make sense (if they don't you might have a bug!). If you were to apply the classifier into a different domain such laptop reviews or restaurant reviews, do you think these features would generalise well? Can you propose what features the classifier could pick up as important in the new domain? (**2 marks**)\n",
    "\n",
    "\n",
    "- Provide well documented and commented code describing all of your choices. In general, you are free to make decisions about text processing (e.g. punctuation, numbers, vocabulary size) and hyperparameter values. We expect to see justifications and discussion for all of your choices (**2 marks**). \n",
    "\n",
    "\n",
    "- Provide efficient solutions by using Numpy arrays when possible (you can find tips in Lab 1 sheet). Executing the whole notebook with your code should not take more than 5 minutes on a any standard computer (e.g. Intel Core i5 CPU, 8 or 16GB RAM) excluding hyperparameter tuning runs (**2 marks**). \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Data \n",
    "\n",
    "The data you will use are taken from here: [http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/) and you can find it in the `./data_sentiment` folder in CSV format:\n",
    "\n",
    "- `data_sentiment/train.csv`: contains 1,400 reviews, 700 positive (label: 1) and 700 negative (label: 0) to be used for training.\n",
    "- `data_sentiment/dev.csv`: contains 200 reviews, 100 positive and 100 negative to be used for hyperparameter selection and monitoring the training process.\n",
    "- `data_sentiment/test.csv`: contains 400 reviews, 200 positive and 200 negative to be used for testing.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Submission Instructions\n",
    "\n",
    "You should submit a Jupyter Notebook file (assignment1.ipynb) and an exported PDF version (you can do it from Jupyter: `File->Download as->PDF via Latex` or you can print it as PDF using your browser).\n",
    "\n",
    "You are advised to follow the code structure given in this notebook by completing all given funtions. You can also write any auxilliary/helper functions (and arguments for the functions) that you might need but note that you can provide a full solution without any such functions. Similarly, you can just use only the packages imported below but you are free to use any functionality from the [Python Standard Library](https://docs.python.org/2/library/index.html), NumPy, SciPy (excluding built-in softmax funtcions) and Pandas. You are not allowed to use any third-party library such as Scikit-learn (apart from metric functions already provided), NLTK, Spacy, Keras etc.. \n",
    "\n",
    "There is no single correct answer on what your accuracy should be, but correct implementations usually achieve F1-scores around 80\\% or higher. The quality of the analysis of the results is as important as the accuracy itself. \n",
    "\n",
    "This assignment will be marked out of 20. It is worth 20\\% of your final grade in the module.\n",
    "\n",
    "The deadline for this assignment is **23:59 on Mon, 14 Mar 2022** and it needs to be submitted via Blackboard. Standard departmental penalties for lateness will be applied. We use a range of strategies to **detect [unfair means](https://www.sheffield.ac.uk/ssid/unfair-means/index)**, including Turnitin which helps detect plagiarism. Use of unfair means would result in getting a failing grade.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tune hyperparameters and show model performance? [Y/N]Y\n"
     ]
    }
   ],
   "source": [
    "tune_params = input(\"tune hyperparameters and show model performance? [Y/N]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "while (tune_params != 'Y') & (tune_params != 'N'):\n",
    "    tune_params = input(\"tune hyperparameters and show model performance? [Y/N]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T13:41:17.642162Z",
     "start_time": "2020-03-27T13:41:16.891940Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import random\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# fixing random seed for reproducibility\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Raw texts and labels into arrays\n",
    "\n",
    "First, you need to load the training, development and test sets from their corresponding CSV files (tip: you can use Pandas dataframes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:28.145788Z",
     "start_time": "2020-02-15T14:17:28.066100Z"
    }
   },
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"data_sentiment/train.csv\")\n",
    "development_data = pd.read_csv(\"data_sentiment/dev.csv\")\n",
    "test_data = pd.read_csv(\"data_sentiment/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use Pandas you can see a sample of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:28.900892Z",
     "start_time": "2020-02-15T14:17:28.891221Z"
    }
   },
   "outputs": [],
   "source": [
    "# training_data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to put the raw texts into Python lists and their corresponding labels into NumPy arrays:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:31.115577Z",
     "start_time": "2020-02-15T14:17:31.108038Z"
    }
   },
   "outputs": [],
   "source": [
    "train_content = training_data.iloc[:,0].to_list()\n",
    "development_content = development_data.iloc[:,0].to_list()\n",
    "test_content = test_data.iloc[:,0].to_list()\n",
    "train_label = training_data.iloc[:,1].to_numpy()\n",
    "development_label = development_data.iloc[:,1].to_numpy()\n",
    "test_label = test_data.iloc[:,1].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Representations of Text \n",
    "\n",
    "\n",
    "To train and test Logisitc Regression models, you first need to obtain vector representations for all documents given a vocabulary of features (unigrams, bigrams, trigrams).\n",
    "\n",
    "\n",
    "## Text Pre-Processing Pipeline\n",
    "\n",
    "To obtain a vocabulary of features, you should: \n",
    "- tokenise all texts into a list of unigrams (tip: using a regular expression) \n",
    "- remove stop words (using the one provided or one of your preference) \n",
    "- compute bigrams, trigrams given the remaining unigrams (or character ngrams from the unigrams)\n",
    "- remove ngrams appearing in less than K documents\n",
    "- use the remaining to create a vocabulary of unigrams, bigrams and trigrams (or character n-grams). You can keep top N if you encounter memory issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:31.860420Z",
     "start_time": "2020-02-15T14:17:31.855439Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = ['a','in','on','at','and','or', \n",
    "              'to', 'the', 'of', 'an', 'by', \n",
    "              'as', 'is', 'was', 'were', 'been', 'be', \n",
    "              'are','for', 'this', 'that', 'these', 'those', 'you', 'i',\n",
    "             'it', 'he', 'she', 'we', 'they', 'will', 'have', 'has',\n",
    "              'do', 'did', 'can', 'could', 'who', 'which', 'what', \n",
    "             'his', 'her', 'they', 'them', 'from', 'with', 'its']\n",
    "stop_words = ['a', 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after','afterwards', 'again', 'against', 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', 'around', 'as', 'aside', 'ask', 'asking', 'associated', 'at', 'available', 'away', 'awfully', 'b', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'believe', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'both', 'brief', 'but', 'by', 'c', 'came', 'can', 'cannot', 'cant', 'cause', 'causes', 'certain', 'certainly', 'changes', 'clearly', 'co', 'com', 'come', 'comes', 'concerning', 'consequently', 'consider', 'considering', 'contain', 'containing', 'contains', 'corresponding', 'could', 'course', 'currently', 'd','definitely', 'described', 'despite', 'did', 'different', 'do', 'does', 'doing', 'done', 'down', 'downwards', 'during', 'e', 'each', 'edu', 'eg', 'eight', 'either', 'else', 'elsewhere', 'enough', 'entirely', 'especially', 'et', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'exactly', 'example', 'except', 'f', 'far', 'few', 'fifth', 'first', 'five', 'followed', 'following', 'follows', 'for', 'former', 'formerly', 'forth', 'four', 'from', 'further', 'furthermore', 'g', 'get', 'gets', 'getting', 'given', 'gives', 'go', 'goes', 'going', 'gone', 'got', 'gotten', 'greetings', 'h', 'had', 'happens', 'hardly', 'has', 'have', 'having', 'he', 'hello', 'help', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'hi', 'him', 'himself', 'his', 'hither', 'hopefully', 'how', 'howbeit', 'however', 'i', 'ie', 'if', 'ignored', 'immediate', 'in', 'inasmuch', 'inc', 'indeed', 'indicate', 'indicated', 'indicates', 'inner', 'insofar', 'instead', 'into', 'inward', 'is', 'it', 'its', 'itself', 'j', 'just', 'k', 'keep', 'keeps', 'kept', 'know', 'knows', 'known', 'l', 'last', 'lately', 'later', 'latter', 'latterly', 'least', 'less', 'lest', 'let', 'like', 'liked', 'likely', 'little', 'look', 'looking', 'looks', 'ltd', 'm', 'mainly', 'many', 'may', 'maybe', 'me', 'mean', 'meanwhile', 'merely','might', 'more', 'moreover', 'most', 'mostly', 'much', 'must', 'my', 'myself', 'n', 'name', 'namely', 'nd','near', 'nearly', 'necessary', 'need', 'needs', 'neither', 'never', 'nevertheless', 'new', 'next', 'nine', 'no', 'nobody', 'non', 'none', 'noone', 'nor', 'normally', 'not', 'note', 'nothing', 'novel', 'now', 'nowhere', 'o', 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay','old', 'on', 'once', 'one', 'ones', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'own', 'p', 'particular', 'particularly', 'per', 'perhaps', 'placed', 'please', 'plus', 'possible', 'presumably', 'probably', 'provides', 'q', 'que', 'quite', 'qv', 'r', 'rather', 'rd', 're', 'really', 'reasonably', 'regarding', 'regardless', 'regards', 'relatively', 'respectively', 'right', 's', 'said', 'same', 'saw', 'say', 'saying', 'says', 'second', 'secondly', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'shall', 'she', 'should', 'since', 'six', 'so', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'sorry', 'specified', 'specify', 'specifying', 'still', 'sub', 'such', 'sup', 'sure', 't', 'take', 'taken', 'tell', 'tends', 'th', 'than', 'thank', 'thanks', 'thanx', 'that', 'thats', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'theres', 'thereupon', 'these', 'they', 'think', 'third', 'this', 'thorough', 'thoroughly', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'truly', 'try', 'trying', 'twice', 'two', 'u', 'un', 'under', 'unfortunately', 'unless', 'unlikely', 'until', 'unto', 'up', 'upon', 'us', 'use', 'used', 'useful', 'uses', 'using', 'usually', 'uucp', 'v', 'value', 'various', 've', 'very', 'via', 'viz', 'vs', 'w', 'want', 'want', 'was', 'way', 'we', 'welcome', 'well', 'went', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon','wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'willing', 'wish', 'with', 'within', 'without', 'wonder', 'would', 'would', 'x', 'y', 'yes', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves', 'z', 'zero']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram extraction from a document\n",
    "\n",
    "You first need to implement the `extract_ngrams` function. It takes as input:\n",
    "- `x_raw`: a string corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `vocab`: a given vocabulary. It should be used to extract specific features.\n",
    "- `char_ngrams`: boolean. If true the function extracts character n-grams\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `x': a list of all extracted features.\n",
    "\n",
    "See the examples below to see how this function should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:33.169090Z",
     "start_time": "2020-02-15T14:17:33.161268Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_ngrams(x_raw, ngram_range=(1,3), token_pattern=r'[A-Za-z]+', \n",
    "                   stop_words=[], vocab=set(), char_ngrams=False):\n",
    "    x = []\n",
    "    x_raw = re.findall(token_pattern, x_raw)\n",
    "    if char_ngrams==True:\n",
    "        x_raw = ''.join(x_raw)\n",
    "        for i in range(ngram_range[1]-ngram_range[0]+1):\n",
    "            for j in range(len(x_raw)-ngram_range[0]-i+1):\n",
    "                x.append([x_raw[j:j+ngram_range[0]+i]])\n",
    "        return x\n",
    "    \n",
    "    words = [word for word in x_raw if word not in stop_words]\n",
    "    for i in range(ngram_range[1]-ngram_range[0]+1):\n",
    "        for j in range(len(words)-ngram_range[0]-i+1):\n",
    "            x.append(words[j:j+ngram_range[0]+i])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it is OK to represent n-grams using lists instead of tuples: e.g. `['great', ['great', 'movie']]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For extracting character n-grams the function should work as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_ngrams(\"movie\", \n",
    "               ngram_range=(2,4), \n",
    "               stop_words=[],\n",
    "               char_ngrams=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vocabulary \n",
    "\n",
    "The `get_vocab` function will be used to (1) create a vocabulary of ngrams; (2) count the document frequencies of ngrams; (3) their raw frequency. It takes as input:\n",
    "- `X_raw`: a list of strings each corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n",
    "- `stop_words`: a list of stop words\n",
    "- `min_df`: keep ngrams with a minimum document frequency.\n",
    "- `keep_topN`: keep top-N more frequent ngrams.\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `vocab`: a set of the n-grams that will be used as features.\n",
    "- `df`: a Counter (or dict) that contains ngrams as keys and their corresponding document frequency as values.\n",
    "- `ngram_counts`: counts of each ngram in vocab\n",
    "\n",
    "Hint: it should make use of the `extract_ngrams` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(X_raw, ngram_range=(1,3), token_pattern=r'\\w+', \n",
    "              min_df=0, keep_topN=0, \n",
    "              stop_words=[],char_ngrams=False):\n",
    "    all_words = extract_ngrams(x_raw=str(X_raw), ngram_range=ngram_range, stop_words=stop_words, char_ngrams=char_ngrams)\n",
    "    \n",
    "    # compute term frequency\n",
    "    ngram_counts = {}\n",
    "    for word in all_words:\n",
    "        temp = ' '.join(word)\n",
    "        if temp not in ngram_counts:\n",
    "            ngram_counts[temp] = 1\n",
    "        else:\n",
    "            ngram_counts[temp] = ngram_counts[temp] + 1\n",
    "    ngram_counts = {key:val for key, val in ngram_counts.items() if val >= keep_topN}\n",
    "    \n",
    "    # compute df by using words in ngram_counts in order to be efficient.\n",
    "    # The reason I can do this is that I only care about the words in ngram_counts instead of all words in the content\n",
    "    df = {}\n",
    "    for i in ngram_counts.keys():\n",
    "        counts = 0\n",
    "        for j in range(len(X_raw)):\n",
    "            if i in X_raw[j]:\n",
    "                counts += 1\n",
    "        df[i] = counts\n",
    "    df = {key:val for key, val in df.items() if val >= min_df}\n",
    "    \n",
    "    # use the words in df to create vocab set\n",
    "    # the reason to choose df is that dict df is smaller than dict ngram_counts and only words in df are valuable.\n",
    "    vocab = [[i] for i in df.keys()]\n",
    "    \n",
    "    return vocab, df, ngram_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should use `get_vocab` to create your vocabulary and get document and raw frequencies of n-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:39.319793Z",
     "start_time": "2020-02-15T14:17:36.836545Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab, df, tf = get_vocab(X_raw=train_content,min_df=55, keep_topN=75, stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you need to create 2 dictionaries: (1) vocabulary id -> word; and  (2) word -> vocabulary id so you can use them for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:39.326811Z",
     "start_time": "2020-02-15T14:17:39.322256Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_to_id = {}\n",
    "id_to_vocab = {}\n",
    "for word in vocab:\n",
    "    string = \" \".join(word)\n",
    "    vocab_to_id[string] = vocab.index(word)\n",
    "    id_to_vocab[vocab.index(word)] = string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should be able to extract n-grams for each text in the training, development and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a content that only consists of words in vocab\n",
    "# because using the original content would affect the performance of bigram, trigram, etc.\n",
    "def content_without_stopwords (raw_content, vocab):\n",
    "    processed_content = [None]*len(raw_content)\n",
    "    for i in range(len(raw_content)):\n",
    "        raw_words = raw_content[i].split()\n",
    "        resultwords  = [word for word in raw_words if [word] in vocab]\n",
    "        processed_content[i] = ' '.join(resultwords)\n",
    "    return processed_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_content = content_without_stopwords(raw_content=train_content, vocab=vocab)\n",
    "processed_dev_content = content_without_stopwords(raw_content=development_content, vocab=vocab)\n",
    "processed_test_content = content_without_stopwords(raw_content=test_content, vocab=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorise documents "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, write a function `vectoriser` to obtain Bag-of-ngram representations for a list of documents. The function should take as input:\n",
    "- `X_ngram`: a list of texts (documents), where each text is represented as list of n-grams in the `vocab`\n",
    "- `vocab`: a set of n-grams to be used for representing the documents\n",
    "\n",
    "and return:\n",
    "- `X_vec`: an array with dimensionality Nx|vocab| where N is the number of documents and |vocab| is the size of the vocabulary. Each element of the array should represent the frequency of a given n-gram in a document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:40.219201Z",
     "start_time": "2020-02-15T14:17:40.215129Z"
    }
   },
   "outputs": [],
   "source": [
    "def vectorise(X_ngram, vocab):\n",
    "    X_vec = np.empty([len(X_ngram), len(vocab)])\n",
    "    for i in range(len(vocab)):\n",
    "        string = \" \".join(vocab[i])\n",
    "        for j in range(len(X_ngram)):\n",
    "            X_vec[j,i] = X_ngram[j].count(string)     \n",
    "    return X_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use `vectorise` to obtain document vectors for each document in the train, development and test set. You should extract both count and tf.idf vectors respectively:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:41.999574Z",
     "start_time": "2020-02-15T14:17:40.376534Z"
    }
   },
   "outputs": [],
   "source": [
    "train_vec = vectorise(X_ngram=processed_train_content, vocab=vocab)\n",
    "development_vec = vectorise(X_ngram=processed_dev_content, vocab=vocab)\n",
    "test_vec = vectorise(X_ngram=processed_test_content, vocab=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF.IDF vectors\n",
    "\n",
    "First compute `idfs` an array containing inverted document frequencies (Note: its elements should correspond to your `vocab`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:42.022692Z",
     "start_time": "2020-02-15T14:17:42.012315Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "idf = {}\n",
    "for key, value in df.items():\n",
    "    idf[key] = math.log(len(train_content)/value,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then transform your count vectors to tf.idf vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:42.802265Z",
     "start_time": "2020-02-15T14:17:42.752448Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a list of idf values\n",
    "idf_list = list(idf.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to compute tfidf because this function would be used many times\n",
    "def compute_tfidf(idf_list, vec):\n",
    "    tfidf = np.empty([(np.shape(vec)[0]), (np.shape(vec)[1])])\n",
    "    for i in range(np.shape(vec)[0]):\n",
    "        for j in range(np.shape(vec)[1]):\n",
    "            tfidf[i,j] = vec[i,j]*idf_list[j]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf = compute_tfidf(idf_list=idf_list, vec=train_vec)\n",
    "development_tfidf = compute_tfidf(idf_list=idf_list, vec=development_vec)\n",
    "test_tfidf = compute_tfidf(idf_list=idf_list, vec=test_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Logistic Regression\n",
    "\n",
    "After obtaining vector representations of the data, now you are ready to implement Binary Logistic Regression for classifying sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to implement the `sigmoid` function. It takes as input:\n",
    "\n",
    "- `z`: a real number or an array of real numbers \n",
    "\n",
    "and returns:\n",
    "\n",
    "- `sig`: the sigmoid of `z`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:44.160661Z",
     "start_time": "2020-02-15T14:17:44.157902Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    sig = 1/(1+np.exp(-z))   \n",
    "    return sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, implement the `predict_proba` function to obtain prediction probabilities. It takes as input:\n",
    "\n",
    "- `X`: an array of inputs, i.e. documents represented by bag-of-ngram vectors ($N \\times |vocab|$)\n",
    "- `weights`: a 1-D array of the model's weights $(1, |vocab|)$\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `preds_proba`: the prediction probabilities of X given the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:44.718566Z",
     "start_time": "2020-02-15T14:17:44.715017Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_proba(X, weights):\n",
    "    z = np.dot(X, weights)\n",
    "    preds_proba = sigmoid(z)\n",
    "    return preds_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, implement the `predict_class` function to obtain the most probable class for each vector in an array of input vectors. It takes as input:\n",
    "\n",
    "- `X`: an array of documents represented by bag-of-ngram vectors ($N \\times |vocab|$)\n",
    "- `weights`: a 1-D array of the model's weights $(1, |vocab|)$\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `preds_class`: the predicted class for each x in X given the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:45.002125Z",
     "start_time": "2020-02-15T14:17:44.998668Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_class(X, weights):\n",
    "    preds_proba = predict_proba(X, weights)\n",
    "    preds_class = np.zeros(len(preds_proba))\n",
    "    for i in range(len(preds_proba)):\n",
    "        if preds_proba[i] < 0.5:\n",
    "            preds_class[i] = 0\n",
    "        else:\n",
    "            preds_class[i] = 1    \n",
    "    return preds_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn the weights from data, we need to minimise the binary cross-entropy loss. Implement `binary_loss` that takes as input:\n",
    "\n",
    "- `X`: input vectors\n",
    "- `Y`: labels\n",
    "- `weights`: model weights\n",
    "- `alpha`: regularisation strength\n",
    "\n",
    "and return:\n",
    "\n",
    "- `l`: the loss score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:45.455533Z",
     "start_time": "2020-02-15T14:17:45.451475Z"
    }
   },
   "outputs": [],
   "source": [
    "# use array to calculate\n",
    "def binary_loss(X, Y, weights, alpha=0.00001):\n",
    "    '''\n",
    "    Binary Cross-entropy Loss\n",
    "\n",
    "    X:(len(X),len(vocab))\n",
    "    Y: array len(Y)\n",
    "    weights: array len(X)\n",
    "    '''\n",
    "    y = predict_proba(X, weights)\n",
    "    l_first = (np.dot(np.transpose(-Y), (np.log(y))) - np.dot(np.transpose(1-Y), (np.log(1-y))))/len(Y)\n",
    "    r = np.dot(np.transpose(weights), weights)\n",
    "    l = l_first + r*alpha\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can implement Stochastic Gradient Descent to learn the weights of your sentiment classifier. The `SGD` function takes as input:\n",
    "\n",
    "- `X_tr`: array of training data (vectors)\n",
    "- `Y_tr`: labels of `X_tr`\n",
    "- `X_dev`: array of development (i.e. validation) data (vectors)\n",
    "- `Y_dev`: labels of `X_dev`\n",
    "- `lr`: learning rate\n",
    "- `alpha`: regularisation strength\n",
    "- `epochs`: number of full passes over the training data\n",
    "- `tolerance`: stop training if the difference between the current and previous validation loss is smaller than a threshold\n",
    "- `print_progress`: flag for printing the training progress (train/validation loss)\n",
    "\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `weights`: the weights learned\n",
    "- `training_loss_history`: an array with the average losses of the whole training set after each epoch\n",
    "- `validation_loss_history`: an array with the average losses of the whole development set after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:45.968510Z",
     "start_time": "2020-02-15T14:17:45.958185Z"
    }
   },
   "outputs": [],
   "source": [
    "def SGD(X_tr, Y_tr, X_dev=[], Y_dev=[], lr=0.0001, \n",
    "        alpha=0.001, epochs=180, \n",
    "        tolerance=0.0000001, print_progress=False):\n",
    "    w = np.zeros(np.shape(X_tr)[1])\n",
    "    index = list(range(len(Y_tr)))\n",
    "    training_loss_history = []\n",
    "    validation_loss_history = []\n",
    "    for i in range(epochs):\n",
    "        random.Random(32*i).shuffle(index)\n",
    "        for j in index:\n",
    "            w = w - lr*(predict_proba(X_tr[j], w)-Y_tr[j])*X_tr[j]\n",
    "    \n",
    "    \n",
    "        l = binary_loss(X_tr, Y_tr, w)\n",
    "        l_val = binary_loss(X_dev, Y_dev, w)\n",
    "        training_loss_history.append(l)\n",
    "        validation_loss_history.append(l_val)\n",
    "        if i>0:\n",
    "            if abs(validation_loss_history[i-1]-validation_loss_history[i])<tolerance:\n",
    "                break\n",
    "        weights = w\n",
    "    \n",
    "    if print_progress==True:\n",
    "        print('train loss: ', training_loss_history)\n",
    "        print('------------------------------------------------')\n",
    "        print('val loss: ', validation_loss_history)\n",
    "    \n",
    "    return weights, training_loss_history, validation_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of hyperparameters' values\n",
    "lr = np.array(np.logspace(-6, -3, 4, endpoint=True))\n",
    "alpha = np.array(np.logspace(-6, -3, 4, endpoint=True))\n",
    "epochs = np.array([30, 75, 120, 165])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Logistic Regression with Count vectors\n",
    "\n",
    "First train the model using SGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max accuracy: 0.8245614035087719\n",
      "lr:  0.001\n",
      "alpha:  1e-06\n",
      "epochs:  30\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  lr = 1e-06  |   alpha = 1e-06    |   alpha = 1e-05    |   alpha = 0.0001   |   alpha = 0.001    |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| epochs = 30  | 0.5739348370927319 | 0.5739348370927319 | 0.5739348370927319 | 0.5739348370927319 |\n",
      "| epochs = 75  | 0.6867167919799498 | 0.6867167919799498 | 0.6867167919799498 | 0.6867167919799498 |\n",
      "| epochs = 120 | 0.7192982456140351 | 0.7192982456140351 | 0.7192982456140351 | 0.7192982456140351 |\n",
      "| epochs = 165 | 0.7343358395989975 | 0.7343358395989975 | 0.7343358395989975 | 0.7343358395989975 |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  lr = 1e-05  |   alpha = 1e-06    |   alpha = 1e-05    |   alpha = 0.0001   |   alpha = 0.001    |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| epochs = 30  | 0.7769423558897243 | 0.7769423558897243 | 0.7769423558897243 | 0.7769423558897243 |\n",
      "| epochs = 75  | 0.7894736842105263 | 0.7894736842105263 | 0.7894736842105263 | 0.7894736842105263 |\n",
      "| epochs = 120 | 0.7869674185463659 | 0.7869674185463659 | 0.7869674185463659 | 0.7869674185463659 |\n",
      "| epochs = 165 | 0.7869674185463659 | 0.7869674185463659 | 0.7869674185463659 | 0.7869674185463659 |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| lr = 0.0001  |   alpha = 1e-06    |   alpha = 1e-05    |   alpha = 0.0001   |   alpha = 0.001    |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| epochs = 30  | 0.8070175438596491 | 0.8070175438596491 | 0.8070175438596491 | 0.8070175438596491 |\n",
      "| epochs = 75  | 0.8095238095238095 | 0.8095238095238095 | 0.8095238095238095 | 0.8095238095238095 |\n",
      "| epochs = 120 | 0.8220551378446115 | 0.8220551378446115 | 0.8220551378446115 | 0.8220551378446115 |\n",
      "| epochs = 165 | 0.8220551378446115 | 0.8220551378446115 | 0.8220551378446115 | 0.8220551378446115 |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  lr = 0.001  |   alpha = 1e-06    |   alpha = 1e-05    |   alpha = 0.0001   |   alpha = 0.001    |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| epochs = 30  | 0.8245614035087719 | 0.8245614035087719 | 0.8245614035087719 | 0.8245614035087719 |\n",
      "| epochs = 75  | 0.8245614035087719 | 0.8245614035087719 | 0.8245614035087719 | 0.8245614035087719 |\n",
      "| epochs = 120 | 0.8195488721804511 | 0.8195488721804511 | 0.8195488721804511 | 0.8195488721804511 |\n",
      "| epochs = 165 | 0.8145363408521303 | 0.8145363408521303 | 0.8145363408521303 | 0.8145363408521303 |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "# to train model using different hyperparameters, including alpha, learning rate, and epochs.\n",
    "\n",
    "# Mistakes here: Use validation set to choose hyperparameters, not test set. After choosing the best hyperparameters,\n",
    "# train again and see how it performs on the test set.\n",
    "if tune_params == 'Y':\n",
    "    # create the table showing performance of each model. Here I use accuracy as the performance.\n",
    "    table_lr0 = PrettyTable([\"lr = {}\".format(lr[0]), \"alpha = {}\".format(alpha[0]), \"alpha = {}\".format(alpha[1])\n",
    "                , \"alpha = {}\".format(alpha[2]), \"alpha = {}\".format(alpha[3])])\n",
    "    table_lr1 = PrettyTable([\"lr = {}\".format(lr[1]), \"alpha = {}\".format(alpha[0]), \"alpha = {}\".format(alpha[1])\n",
    "                , \"alpha = {}\".format(alpha[2]), \"alpha = {}\".format(alpha[3])])\n",
    "    table_lr2 = PrettyTable([\"lr = {}\".format(lr[2]), \"alpha = {}\".format(alpha[0]), \"alpha = {}\".format(alpha[1])\n",
    "                , \"alpha = {}\".format(alpha[2]), \"alpha = {}\".format(alpha[3])])\n",
    "    table_lr3 = PrettyTable([\"lr = {}\".format(lr[3]), \"alpha = {}\".format(alpha[0]), \"alpha = {}\".format(alpha[1])\n",
    "                , \"alpha = {}\".format(alpha[2]), \"alpha = {}\".format(alpha[3])])\n",
    "    acc_history = []\n",
    "    # use for-loop to run each model.\n",
    "    for i in range(len(lr)):\n",
    "        for j in range(len(alpha)):\n",
    "            for k in range(len(epochs)):\n",
    "                w_count, t, v = SGD(X_tr=train_vec, Y_tr=train_label, X_dev=development_vec, Y_dev=development_label, lr=lr[i], alpha=alpha[j], epochs=epochs[k])\n",
    "                preds_te_count = predict_class(test_vec, w_count)\n",
    "                if v[len(v)-1] < 1:\n",
    "                    acc_history.append(accuracy_score(test_label,preds_te_count))\n",
    "                else:\n",
    "                    acc_history.append(0)\n",
    "        # add the accuracy into my tables\n",
    "        if i == 0:\n",
    "            table_lr0.add_row([\"epochs = {}\".format(epochs[0]), acc_history[i*16+0], acc_history[i*16+4], acc_history[i*16+8], acc_history[i*16+12]])\n",
    "            table_lr0.add_row([\"epochs = {}\".format(epochs[1]), acc_history[i*16+1], acc_history[i*16+5], acc_history[i*16+9], acc_history[i*16+13]])\n",
    "            table_lr0.add_row([\"epochs = {}\".format(epochs[2]), acc_history[i*16+2], acc_history[i*16+6], acc_history[i*16+10], acc_history[i*16+14]])\n",
    "            table_lr0.add_row([\"epochs = {}\".format(epochs[3]), acc_history[i*16+3], acc_history[i*16+7], acc_history[i*16+11], acc_history[i*16+15]])\n",
    "        if i == 1:\n",
    "            table_lr1.add_row([\"epochs = {}\".format(epochs[0]), acc_history[i*16+0], acc_history[i*16+4], acc_history[i*16+8], acc_history[i*16+12]])\n",
    "            table_lr1.add_row([\"epochs = {}\".format(epochs[1]), acc_history[i*16+1], acc_history[i*16+5], acc_history[i*16+9], acc_history[i*16+13]])\n",
    "            table_lr1.add_row([\"epochs = {}\".format(epochs[2]), acc_history[i*16+2], acc_history[i*16+6], acc_history[i*16+10], acc_history[i*16+14]])\n",
    "            table_lr1.add_row([\"epochs = {}\".format(epochs[3]), acc_history[i*16+3], acc_history[i*16+7], acc_history[i*16+11], acc_history[i*16+15]])\n",
    "        if i == 2:\n",
    "            table_lr2.add_row([\"epochs = {}\".format(epochs[0]), acc_history[i*16+0], acc_history[i*16+4], acc_history[i*16+8], acc_history[i*16+12]])\n",
    "            table_lr2.add_row([\"epochs = {}\".format(epochs[1]), acc_history[i*16+1], acc_history[i*16+5], acc_history[i*16+9], acc_history[i*16+13]])\n",
    "            table_lr2.add_row([\"epochs = {}\".format(epochs[2]), acc_history[i*16+2], acc_history[i*16+6], acc_history[i*16+10], acc_history[i*16+14]])\n",
    "            table_lr2.add_row([\"epochs = {}\".format(epochs[3]), acc_history[i*16+3], acc_history[i*16+7], acc_history[i*16+11], acc_history[i*16+15]])\n",
    "        if i == 3:\n",
    "            table_lr3.add_row([\"epochs = {}\".format(epochs[0]), acc_history[i*16+0], acc_history[i*16+4], acc_history[i*16+8], acc_history[i*16+12]])\n",
    "            table_lr3.add_row([\"epochs = {}\".format(epochs[1]), acc_history[i*16+1], acc_history[i*16+5], acc_history[i*16+9], acc_history[i*16+13]])\n",
    "            table_lr3.add_row([\"epochs = {}\".format(epochs[2]), acc_history[i*16+2], acc_history[i*16+6], acc_history[i*16+10], acc_history[i*16+14]])\n",
    "            table_lr3.add_row([\"epochs = {}\".format(epochs[3]), acc_history[i*16+3], acc_history[i*16+7], acc_history[i*16+11], acc_history[i*16+15]])\n",
    "    result = max(acc_history)\n",
    "    print('max accuracy:', result)\n",
    "    index_of_result = acc_history.index(result)\n",
    "    lr_index = (index_of_result)//16\n",
    "    alpha_index = (index_of_result%16)//4\n",
    "    epochs_index = (index_of_result%4)\n",
    "    print('lr: ', lr[lr_index])\n",
    "    print('alpha: ', alpha[alpha_index])\n",
    "    print('epochs: ', epochs[epochs_index])\n",
    "    print(table_lr0)\n",
    "    print(table_lr1)\n",
    "    print(table_lr2)\n",
    "    print(table_lr3)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above shows that if the learning rate is small, the model needs a lot of training(epochs) to have a better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  [0.5237446231674533, 0.4598958396943405, 0.417625139762551, 0.39131659817128767, 0.3699847163197355, 0.3542820166760697, 0.33979873888380074, 0.3261376559252188, 0.3132001307441163, 0.3123859198456283, 0.29517765831364134, 0.2864944463425897, 0.27930319008959525, 0.28170348646284504, 0.2671498248065886, 0.2621059070939449, 0.2633544590032282, 0.2497121788012522, 0.2451044812534158, 0.24079316510853427, 0.2434685396017481, 0.23631375180526418, 0.22893170952370084, 0.22523144680973398, 0.22126867620071952, 0.21791808774402951, 0.22086096306930622, 0.21159902472315276, 0.2088484826319793, 0.20590085801287555]\n",
      "------------------------------------------------\n",
      "val loss:  [0.578607392784173, 0.5528542875089766, 0.528184863434301, 0.5232283407223768, 0.5121228999596672, 0.5182959445165406, 0.5127358809343335, 0.5057591012220601, 0.5011817186911576, 0.5095155258594161, 0.5008169775709146, 0.49481775392802796, 0.49526182218644393, 0.5036289055091067, 0.4937563872055492, 0.49617816835467193, 0.5082344376684081, 0.4960988775376062, 0.4967311056685078, 0.49730780530622326, 0.5071129271308218, 0.5052426025109235, 0.4981088618185785, 0.5005587753279394, 0.5006436754512664, 0.5017202407598437, 0.5093331363617594, 0.502325959591172, 0.5000915070042214, 0.499669948276321]\n"
     ]
    }
   ],
   "source": [
    "# if choose to tune parameters, show the best model; otherwise, use the default value to train the model.\n",
    "if tune_params == 'Y':    \n",
    "    w_count, t, v = SGD(train_vec, train_label, X_dev=development_vec, Y_dev=development_label, \n",
    "                    lr=lr[lr_index], alpha=alpha[alpha_index], epochs=epochs[epochs_index], print_progress=True)\n",
    "else:\n",
    "    w_count, t, v = SGD(train_vec, train_label, X_dev=development_vec, Y_dev=development_label, print_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the training and validation history per epoch for the best hyperparameter combination. Does your model underfit, overfit or is it about right? Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:51.598911Z",
     "start_time": "2020-02-15T14:17:51.482307Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1524b6ef9d0>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(t, 'r')\n",
    "plt.plot(v, 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T16:52:26.583150Z",
     "start_time": "2020-01-21T16:52:26.578754Z"
    }
   },
   "source": [
    "The final val loss is about 0.5, which is acceptable.\n",
    "The curve of val loss is descending without a trend of ascending. This means the model is neither overfitting nor underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| BOW-count | Precision  | Recall  | F1-Score  |\n",
    "|:-:|:-:|:-:|:-:|\n",
    "| lr = 0.0001  |   |   |   |\n",
    "| BOW-tfidf  |   |   |   |\n",
    "| BOCN-count  |   |   |   |\n",
    "| BOCN-tfidf  |   |   |   |\n",
    "| BOW+BOCN  |   |   |   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "Compute accuracy, precision, recall and F1-scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:51.607940Z",
     "start_time": "2020-02-15T14:17:51.600272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8245614035087719\n",
      "Precision: 0.8177339901477833\n",
      "Recall: 0.8341708542713567\n",
      "F1-Score: 0.8258706467661691\n"
     ]
    }
   ],
   "source": [
    "preds_te_count = predict_class(test_vec, w_count)\n",
    "print('Accuracy:', accuracy_score(test_label,preds_te_count))\n",
    "print('Precision:', precision_score(test_label,preds_te_count))\n",
    "print('Recall:', recall_score(test_label,preds_te_count))\n",
    "print('F1-Score:', f1_score(test_label,preds_te_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, print the top-10 words for the negative and positive class respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:51.613935Z",
     "start_time": "2020-02-15T14:17:51.610660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad\n",
      "supposed\n",
      "worst\n",
      "waste\n",
      "boring\n",
      "attempt\n",
      "ridiculous\n",
      "script\n",
      "write\n",
      "stupid\n"
     ]
    }
   ],
   "source": [
    "top_neg = w_count.argsort()[:10]\n",
    "for i in top_neg:\n",
    "    print(id_to_vocab[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T14:17:51.624122Z",
     "start_time": "2020-02-15T14:17:51.615674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perfect\n",
      "hilarious\n",
      "excellent\n",
      "enjoy\n",
      "simple\n",
      "great\n",
      "true\n",
      "memorable\n",
      "movies\n",
      "bit\n"
     ]
    }
   ],
   "source": [
    "top_pos = w_count.argsort()[::-1][:10]\n",
    "for i in top_pos:\n",
    "    print(id_to_vocab[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words 'bad', 'worst', 'waste', etc are used to express the negative feelings of a movie. The words 'perfect', 'enjoy', 'excellent', etc are used to express the positive feelings of a movie. As a result, this model is quite excellent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to apply the classifier we've learned into a different domain such laptop reviews or restaurant reviews, do you think these features would generalise well? Can you propose what features the classifier could pick up as important in the new domain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS:\n",
    "No, the words used to express positive or negative feelings about a movie is different from those about a restaurant or laptop. For the restaurant reviews, words such as delicious, friendly, dirty, etc may be the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss how did you choose model hyperparameters (e.g. learning rate and regularisation strength)? What is the relation between training epochs and learning rate? How the regularisation strength affects performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I create lists for learning rate, alpha, and epochs, each with four values. Then, use for loop to train the models and record the accuracy for each model if the final val loss is under 1. Last, pick up the model with the highest accuracy. The reason I use accuracy to choose the model is that both true positive and true negative are important in this model.Both of the positive reviews and negative reviews are important so the accuracy is what I need\n",
    "\n",
    "If learning rate is small, the epochs needs to be large enough since the weight only changes a little during each epoch. If the learning rate is large, the epochs can be smaller. However, a learning rate that is too large results in unstable training and a learning rate that is too small results in failed training.\n",
    "\n",
    "The larger the regularisation strength is, the bigger penalty term is. If the regularisation strength is near 0, the penalty term doesn't work, which means the model has a high possibility of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Logistic Regression with TF.IDF vectors\n",
    "\n",
    "Follow the same steps as above (i.e. evaluating count n-gram representations).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW-tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max accuracy: 0.8421052631578947\n",
      "lr:  0.001\n",
      "alpha:  1e-06\n",
      "epochs:  30\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  lr = 1e-06  |   alpha = 1e-06    |   alpha = 1e-05    |   alpha = 0.0001   |   alpha = 0.001    |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| epochs = 30  | 0.6090225563909775 | 0.6090225563909775 | 0.6090225563909775 | 0.6090225563909775 |\n",
      "| epochs = 75  | 0.6265664160401002 | 0.6265664160401002 | 0.6265664160401002 | 0.6265664160401002 |\n",
      "| epochs = 120 | 0.6491228070175439 | 0.6491228070175439 | 0.6491228070175439 | 0.6491228070175439 |\n",
      "| epochs = 165 | 0.6691729323308271 | 0.6691729323308271 | 0.6691729323308271 | 0.6691729323308271 |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  lr = 1e-05  |   alpha = 1e-06    |   alpha = 1e-05    |   alpha = 0.0001   |   alpha = 0.001    |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| epochs = 30  | 0.7243107769423559 | 0.7243107769423559 | 0.7243107769423559 | 0.7243107769423559 |\n",
      "| epochs = 75  | 0.7694235588972431 | 0.7694235588972431 | 0.7694235588972431 | 0.7694235588972431 |\n",
      "| epochs = 120 | 0.7794486215538847 | 0.7794486215538847 | 0.7794486215538847 | 0.7794486215538847 |\n",
      "| epochs = 165 | 0.7894736842105263 | 0.7894736842105263 | 0.7894736842105263 | 0.7894736842105263 |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| lr = 0.0001  |   alpha = 1e-06    |   alpha = 1e-05    |   alpha = 0.0001   |   alpha = 0.001    |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| epochs = 30  | 0.8020050125313283 | 0.8020050125313283 | 0.8020050125313283 | 0.8020050125313283 |\n",
      "| epochs = 75  | 0.8245614035087719 | 0.8245614035087719 | 0.8245614035087719 | 0.8245614035087719 |\n",
      "| epochs = 120 | 0.8370927318295739 | 0.8370927318295739 | 0.8370927318295739 | 0.8370927318295739 |\n",
      "| epochs = 165 | 0.8370927318295739 | 0.8370927318295739 | 0.8370927318295739 | 0.8370927318295739 |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  lr = 0.001  |   alpha = 1e-06    |   alpha = 1e-05    |   alpha = 0.0001   |   alpha = 0.001    |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| epochs = 30  | 0.8421052631578947 | 0.8421052631578947 | 0.8421052631578947 | 0.8421052631578947 |\n",
      "| epochs = 75  | 0.8421052631578947 | 0.8421052631578947 | 0.8421052631578947 | 0.8421052631578947 |\n",
      "| epochs = 120 | 0.8370927318295739 | 0.8370927318295739 | 0.8370927318295739 | 0.8370927318295739 |\n",
      "| epochs = 165 | 0.8421052631578947 | 0.8421052631578947 | 0.8421052631578947 | 0.8421052631578947 |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "if tune_params == 'Y':\n",
    "    acc_history = []\n",
    "    table_lr0 = PrettyTable([\"lr = {}\".format(lr[0]), \"alpha = {}\".format(alpha[0]), \"alpha = {}\".format(alpha[1])\n",
    "                , \"alpha = {}\".format(alpha[2]), \"alpha = {}\".format(alpha[3])])\n",
    "    table_lr1 = PrettyTable([\"lr = {}\".format(lr[1]), \"alpha = {}\".format(alpha[0]), \"alpha = {}\".format(alpha[1])\n",
    "                , \"alpha = {}\".format(alpha[2]), \"alpha = {}\".format(alpha[3])])\n",
    "    table_lr2 = PrettyTable([\"lr = {}\".format(lr[2]), \"alpha = {}\".format(alpha[0]), \"alpha = {}\".format(alpha[1])\n",
    "                , \"alpha = {}\".format(alpha[2]), \"alpha = {}\".format(alpha[3])])\n",
    "    table_lr3 = PrettyTable([\"lr = {}\".format(lr[3]), \"alpha = {}\".format(alpha[0]), \"alpha = {}\".format(alpha[1])\n",
    "                , \"alpha = {}\".format(alpha[2]), \"alpha = {}\".format(alpha[3])])\n",
    "    for i in range(len(lr)):\n",
    "        for j in range(len(alpha)):\n",
    "            for k in range(len(epochs)):\n",
    "                w_tfidf, t_tfidf, v_tfidf = SGD(train_tfidf, train_label, X_dev=development_tfidf, Y_dev=development_label, lr=lr[i], alpha=alpha[j], epochs=epochs[k])\n",
    "                preds_te_count = predict_class(test_tfidf, w_tfidf)\n",
    "                if v_tfidf[len(v_tfidf)-1] < 1:\n",
    "                    acc_history.append(accuracy_score(test_label,preds_te_count))\n",
    "                else:\n",
    "                    acc_history.append(0)\n",
    "        if i == 0:\n",
    "            table_lr0.add_row([\"epochs = {}\".format(epochs[0]), acc_history[i*16+0], acc_history[i*16+4], acc_history[i*16+8], acc_history[i*16+12]])\n",
    "            table_lr0.add_row([\"epochs = {}\".format(epochs[1]), acc_history[i*16+1], acc_history[i*16+5], acc_history[i*16+9], acc_history[i*16+13]])\n",
    "            table_lr0.add_row([\"epochs = {}\".format(epochs[2]), acc_history[i*16+2], acc_history[i*16+6], acc_history[i*16+10], acc_history[i*16+14]])\n",
    "            table_lr0.add_row([\"epochs = {}\".format(epochs[3]), acc_history[i*16+3], acc_history[i*16+7], acc_history[i*16+11], acc_history[i*16+15]])\n",
    "        if i == 1:\n",
    "            table_lr1.add_row([\"epochs = {}\".format(epochs[0]), acc_history[i*16+0], acc_history[i*16+4], acc_history[i*16+8], acc_history[i*16+12]])\n",
    "            table_lr1.add_row([\"epochs = {}\".format(epochs[1]), acc_history[i*16+1], acc_history[i*16+5], acc_history[i*16+9], acc_history[i*16+13]])\n",
    "            table_lr1.add_row([\"epochs = {}\".format(epochs[2]), acc_history[i*16+2], acc_history[i*16+6], acc_history[i*16+10], acc_history[i*16+14]])\n",
    "            table_lr1.add_row([\"epochs = {}\".format(epochs[3]), acc_history[i*16+3], acc_history[i*16+7], acc_history[i*16+11], acc_history[i*16+15]])\n",
    "        if i == 2:\n",
    "            table_lr2.add_row([\"epochs = {}\".format(epochs[0]), acc_history[i*16+0], acc_history[i*16+4], acc_history[i*16+8], acc_history[i*16+12]])\n",
    "            table_lr2.add_row([\"epochs = {}\".format(epochs[1]), acc_history[i*16+1], acc_history[i*16+5], acc_history[i*16+9], acc_history[i*16+13]])\n",
    "            table_lr2.add_row([\"epochs = {}\".format(epochs[2]), acc_history[i*16+2], acc_history[i*16+6], acc_history[i*16+10], acc_history[i*16+14]])\n",
    "            table_lr2.add_row([\"epochs = {}\".format(epochs[3]), acc_history[i*16+3], acc_history[i*16+7], acc_history[i*16+11], acc_history[i*16+15]])\n",
    "        if i == 3:\n",
    "            table_lr3.add_row([\"epochs = {}\".format(epochs[0]), acc_history[i*16+0], acc_history[i*16+4], acc_history[i*16+8], acc_history[i*16+12]])\n",
    "            table_lr3.add_row([\"epochs = {}\".format(epochs[1]), acc_history[i*16+1], acc_history[i*16+5], acc_history[i*16+9], acc_history[i*16+13]])\n",
    "            table_lr3.add_row([\"epochs = {}\".format(epochs[2]), acc_history[i*16+2], acc_history[i*16+6], acc_history[i*16+10], acc_history[i*16+14]])\n",
    "            table_lr3.add_row([\"epochs = {}\".format(epochs[3]), acc_history[i*16+3], acc_history[i*16+7], acc_history[i*16+11], acc_history[i*16+15]])\n",
    "    result = max(acc_history)\n",
    "    print('max accuracy:', result)\n",
    "    index_of_result = acc_history.index(result)\n",
    "    lr_index = (index_of_result)//16\n",
    "    alpha_index = (index_of_result%16)//4\n",
    "    epochs_index = (index_of_result%4)\n",
    "    print('lr: ', lr[lr_index])\n",
    "    print('alpha: ', alpha[alpha_index])\n",
    "    print('epochs: ', epochs[epochs_index])\n",
    "    print(table_lr0)\n",
    "    print(table_lr1)\n",
    "    print(table_lr2)\n",
    "    print(table_lr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  [0.5821423604447067, 0.5191824525351423, 0.4771485270391641, 0.4455105209681702, 0.4206253516438873, 0.4004119687788791, 0.3834264214319399, 0.36873649830226524, 0.3559369071707424, 0.34458945221997633, 0.33442878925625347, 0.3251911145364116, 0.31680218695582163, 0.30913201678853824, 0.30205401863364145, 0.29561387714164516, 0.289374353545951, 0.28366865282050013, 0.2783334226875282, 0.2731890999786553, 0.26847640662830324, 0.2642349730304086, 0.2598222304485384, 0.25554997408348923, 0.25167316304893955, 0.24800368139970355, 0.24449192831281358, 0.24118190275631513, 0.2378454268603819, 0.23487269208847114]\n",
      "------------------------------------------------\n",
      "val loss:  [0.6188721607708753, 0.5797247672149108, 0.5554238039052309, 0.5379952194868945, 0.5250780603283466, 0.5154174859931234, 0.5076522389648609, 0.5010153830232392, 0.4955418407020135, 0.4915979604982202, 0.4878159080322553, 0.4843017757290691, 0.48141259526981806, 0.47906874017566115, 0.47705445595394064, 0.47567849357640823, 0.4738620150442363, 0.4723020879546861, 0.4717916307729878, 0.47053285033939973, 0.47005192421152897, 0.4699817529210617, 0.4693524439525947, 0.46849659303745517, 0.4682221147447083, 0.46808057061172914, 0.46807520409825015, 0.4685228947778664, 0.46833299473138323, 0.46827970845677463]\n"
     ]
    }
   ],
   "source": [
    "if tune_params == 'Y':\n",
    "    w_tfidf, t_tfidf, v_tfidf = SGD(train_tfidf, train_label, X_dev=development_tfidf, Y_dev=development_label, \n",
    "                                lr=lr[lr_index], alpha=alpha[alpha_index], epochs=epochs[epochs_index], print_progress=True)\n",
    "else:\n",
    "    w_tfidf, t_tfidf, v_tfidf = SGD(train_tfidf, train_label, X_dev=development_tfidf, Y_dev=development_label, print_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1524b784550>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(t_tfidf, 'r')\n",
    "plt.plot(v_tfidf, 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final val loss is about 0.5, which is acceptable.\n",
    "The curve of val loss is descending without a trend of ascending. This means the model is neither overfitting nor underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8421052631578947\n",
      "Precision: 0.8333333333333334\n",
      "Recall: 0.8542713567839196\n",
      "F1-Score: 0.8436724565756824\n"
     ]
    }
   ],
   "source": [
    "preds_te_count = predict_class(test_tfidf, w_tfidf)\n",
    "print('Accuracy:', accuracy_score(test_label,preds_te_count))\n",
    "print('Precision:', precision_score(test_label,preds_te_count))\n",
    "print('Recall:', recall_score(test_label,preds_te_count))\n",
    "print('F1-Score:', f1_score(test_label,preds_te_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad\n",
      "worst\n",
      "supposed\n",
      "boring\n",
      "waste\n",
      "ridiculous\n",
      "awful\n",
      "fails\n",
      "script\n",
      "filmmakers\n"
     ]
    }
   ],
   "source": [
    "top_neg = w_tfidf.argsort()[:10]\n",
    "for i in top_neg:\n",
    "    print(id_to_vocab[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perfect\n",
      "hilarious\n",
      "great\n",
      "excellent\n",
      "terrific\n",
      "memorable\n",
      "simple\n",
      "perfectly\n",
      "world\n",
      "enjoyed\n"
     ]
    }
   ],
   "source": [
    "top_pos = w_tfidf.argsort()[::-1][:10]\n",
    "for i in top_pos:\n",
    "    print(id_to_vocab[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words 'bad', 'worst', 'boring', etc are used to express the negative feelings of a movie. The words 'perfect', 'terrific', 'memorable', etc are used to express the positive feelings of a movie. As a result, this model is quite excellent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now repeat the training and evaluation process for BOW-tfidf, BOCN-count, BOCN-tfidf, BOW+BOCN including hyperparameter tuning for each model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOCN-count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_bocn, df_bocn, tf_bocn = get_vocab(X_raw=train_content,min_df=55, keep_topN=75, char_ngrams=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_bocn_to_id = {}\n",
    "id_to_vocab_bocn = {}\n",
    "for word in vocab_bocn:\n",
    "    string = \" \".join(word)\n",
    "    vocab_bocn_to_id[string] = vocab_bocn.index(word)\n",
    "    id_to_vocab_bocn[vocab_bocn.index(word)] = string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec_bocn = vectorise(X_ngram=processed_train_content, vocab=vocab_bocn)\n",
    "development_vec_bocn = vectorise(X_ngram=processed_dev_content, vocab=vocab_bocn)\n",
    "test_vec_bocn = vectorise(X_ngram=processed_test_content, vocab=vocab_bocn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "idf_bocn = {}\n",
    "for key, value in df_bocn.items():\n",
    "    idf_bocn[key] = math.log(len(train_content)/value,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_bocn_list = list(idf_bocn.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf_bocn = compute_tfidf(idf_list=idf_bocn_list, vec=train_vec_bocn)\n",
    "development_tfidf_bocn = compute_tfidf(idf_list=idf_bocn_list, vec=development_vec_bocn)\n",
    "test_tfidf_bocn = compute_tfidf(idf_list=idf_bocn_list, vec=test_vec_bocn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-1c73d9e9e108>:11: RuntimeWarning: divide by zero encountered in log\n",
      "  l_first = (np.dot(np.transpose(-Y), (np.log(y))) - np.dot(np.transpose(1-Y), (np.log(1-y))))/len(Y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max accuracy: 0.7869674185463659\n",
      "lr:  1e-05\n",
      "alpha:  1e-06\n",
      "epochs:  120\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  lr = 1e-06  |   alpha = 1e-06    |   alpha = 1e-05    |   alpha = 0.0001   |   alpha = 0.001    |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| epochs = 30  | 0.7042606516290727 | 0.7042606516290727 | 0.7042606516290727 | 0.7042606516290727 |\n",
      "| epochs = 75  | 0.7268170426065163 | 0.7268170426065163 | 0.7268170426065163 | 0.7268170426065163 |\n",
      "| epochs = 120 | 0.7343358395989975 | 0.7343358395989975 | 0.7343358395989975 | 0.7343358395989975 |\n",
      "| epochs = 165 | 0.7418546365914787 | 0.7418546365914787 | 0.7418546365914787 | 0.7418546365914787 |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  lr = 1e-05  |   alpha = 1e-06    |   alpha = 1e-05    |   alpha = 0.0001   |   alpha = 0.001    |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| epochs = 30  | 0.7619047619047619 | 0.7619047619047619 | 0.7619047619047619 | 0.7619047619047619 |\n",
      "| epochs = 75  | 0.7644110275689223 | 0.7644110275689223 | 0.7644110275689223 | 0.7644110275689223 |\n",
      "| epochs = 120 | 0.7869674185463659 | 0.7869674185463659 | 0.7869674185463659 | 0.7869674185463659 |\n",
      "| epochs = 165 | 0.7719298245614035 | 0.7719298245614035 | 0.7719298245614035 | 0.7719298245614035 |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| lr = 0.0001  |   alpha = 1e-06    |   alpha = 1e-05    |   alpha = 0.0001   |   alpha = 0.001    |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| epochs = 30  | 0.7619047619047619 | 0.7619047619047619 | 0.7619047619047619 | 0.7619047619047619 |\n",
      "| epochs = 75  | 0.7819548872180451 | 0.7819548872180451 | 0.7819548872180451 | 0.7819548872180451 |\n",
      "| epochs = 120 | 0.7744360902255639 | 0.7744360902255639 | 0.7744360902255639 | 0.7744360902255639 |\n",
      "| epochs = 165 | 0.7794486215538847 | 0.7794486215538847 | 0.7794486215538847 | 0.7794486215538847 |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "+--------------+---------------+---------------+----------------+---------------+\n",
      "|  lr = 0.001  | alpha = 1e-06 | alpha = 1e-05 | alpha = 0.0001 | alpha = 0.001 |\n",
      "+--------------+---------------+---------------+----------------+---------------+\n",
      "| epochs = 30  |       0       |       0       |       0        |       0       |\n",
      "| epochs = 75  |       0       |       0       |       0        |       0       |\n",
      "| epochs = 120 |       0       |       0       |       0        |       0       |\n",
      "| epochs = 165 |       0       |       0       |       0        |       0       |\n",
      "+--------------+---------------+---------------+----------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "if tune_params == 'Y':\n",
    "    acc_history = []\n",
    "    table_lr0 = PrettyTable([\"lr = {}\".format(lr[0]), \"alpha = {}\".format(alpha[0]), \"alpha = {}\".format(alpha[1])\n",
    "                , \"alpha = {}\".format(alpha[2]), \"alpha = {}\".format(alpha[3])])\n",
    "    table_lr1 = PrettyTable([\"lr = {}\".format(lr[1]), \"alpha = {}\".format(alpha[0]), \"alpha = {}\".format(alpha[1])\n",
    "                , \"alpha = {}\".format(alpha[2]), \"alpha = {}\".format(alpha[3])])\n",
    "    table_lr2 = PrettyTable([\"lr = {}\".format(lr[2]), \"alpha = {}\".format(alpha[0]), \"alpha = {}\".format(alpha[1])\n",
    "                , \"alpha = {}\".format(alpha[2]), \"alpha = {}\".format(alpha[3])])\n",
    "    table_lr3 = PrettyTable([\"lr = {}\".format(lr[3]), \"alpha = {}\".format(alpha[0]), \"alpha = {}\".format(alpha[1])\n",
    "                , \"alpha = {}\".format(alpha[2]), \"alpha = {}\".format(alpha[3])])\n",
    "    for i in range(len(lr)):\n",
    "        for j in range(len(alpha)):\n",
    "            for k in range(len(epochs)):\n",
    "                w_count_bocn, t_bocn, v_bocn = SGD(train_vec_bocn, train_label, X_dev=development_vec_bocn, Y_dev=development_label, lr=lr[i], alpha=alpha[j], epochs=epochs[k])\n",
    "                preds_te_count = predict_class(test_vec_bocn, w_count_bocn)\n",
    "                if v_bocn[len(v_bocn)-1] < 1:\n",
    "                    acc_history.append(accuracy_score(test_label,preds_te_count))\n",
    "                else:\n",
    "                    acc_history.append(0)\n",
    "        if i == 0:\n",
    "            table_lr0.add_row([\"epochs = {}\".format(epochs[0]), acc_history[i*16+0], acc_history[i*16+4], acc_history[i*16+8], acc_history[i*16+12]])\n",
    "            table_lr0.add_row([\"epochs = {}\".format(epochs[1]), acc_history[i*16+1], acc_history[i*16+5], acc_history[i*16+9], acc_history[i*16+13]])\n",
    "            table_lr0.add_row([\"epochs = {}\".format(epochs[2]), acc_history[i*16+2], acc_history[i*16+6], acc_history[i*16+10], acc_history[i*16+14]])\n",
    "            table_lr0.add_row([\"epochs = {}\".format(epochs[3]), acc_history[i*16+3], acc_history[i*16+7], acc_history[i*16+11], acc_history[i*16+15]])\n",
    "        if i == 1:\n",
    "            table_lr1.add_row([\"epochs = {}\".format(epochs[0]), acc_history[i*16+0], acc_history[i*16+4], acc_history[i*16+8], acc_history[i*16+12]])\n",
    "            table_lr1.add_row([\"epochs = {}\".format(epochs[1]), acc_history[i*16+1], acc_history[i*16+5], acc_history[i*16+9], acc_history[i*16+13]])\n",
    "            table_lr1.add_row([\"epochs = {}\".format(epochs[2]), acc_history[i*16+2], acc_history[i*16+6], acc_history[i*16+10], acc_history[i*16+14]])\n",
    "            table_lr1.add_row([\"epochs = {}\".format(epochs[3]), acc_history[i*16+3], acc_history[i*16+7], acc_history[i*16+11], acc_history[i*16+15]])\n",
    "        if i == 2:\n",
    "            table_lr2.add_row([\"epochs = {}\".format(epochs[0]), acc_history[i*16+0], acc_history[i*16+4], acc_history[i*16+8], acc_history[i*16+12]])\n",
    "            table_lr2.add_row([\"epochs = {}\".format(epochs[1]), acc_history[i*16+1], acc_history[i*16+5], acc_history[i*16+9], acc_history[i*16+13]])\n",
    "            table_lr2.add_row([\"epochs = {}\".format(epochs[2]), acc_history[i*16+2], acc_history[i*16+6], acc_history[i*16+10], acc_history[i*16+14]])\n",
    "            table_lr2.add_row([\"epochs = {}\".format(epochs[3]), acc_history[i*16+3], acc_history[i*16+7], acc_history[i*16+11], acc_history[i*16+15]])\n",
    "        if i == 3:\n",
    "            table_lr3.add_row([\"epochs = {}\".format(epochs[0]), acc_history[i*16+0], acc_history[i*16+4], acc_history[i*16+8], acc_history[i*16+12]])\n",
    "            table_lr3.add_row([\"epochs = {}\".format(epochs[1]), acc_history[i*16+1], acc_history[i*16+5], acc_history[i*16+9], acc_history[i*16+13]])\n",
    "            table_lr3.add_row([\"epochs = {}\".format(epochs[2]), acc_history[i*16+2], acc_history[i*16+6], acc_history[i*16+10], acc_history[i*16+14]])\n",
    "            table_lr3.add_row([\"epochs = {}\".format(epochs[3]), acc_history[i*16+3], acc_history[i*16+7], acc_history[i*16+11], acc_history[i*16+15]])\n",
    "    result = max(acc_history)\n",
    "    print('max accuracy:', result)\n",
    "    index_of_result = acc_history.index(result)\n",
    "    lr_index = (index_of_result)//16\n",
    "    alpha_index = (index_of_result%16)//4\n",
    "    epochs_index = (index_of_result%4)\n",
    "    print('lr: ', lr[lr_index])\n",
    "    print('alpha: ', alpha[alpha_index])\n",
    "    print('epochs: ', epochs[epochs_index])\n",
    "    print(table_lr0)\n",
    "    print(table_lr1)\n",
    "    print(table_lr2)\n",
    "    print(table_lr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  [0.6586678865543996, 0.6409576538108084, 0.614961752041814, 0.6099612622626234, 0.6050659590053403, 0.5941109401285373, 0.5721538260755585, 0.6165177248681937, 0.566789992670101, 0.551701504992079, 0.5748317519206638, 0.5565582981160806, 0.5505612337997011, 0.5310324752930027, 0.53272522450547, 0.5204643130489307, 0.5195529458636964, 0.51419421594282, 0.5115296745251963, 0.5053134249491735, 0.5402928891318021, 0.49891279946082934, 0.49711488254886027, 0.502233472414653, 0.4900059660727521, 0.5120546353977561, 0.4915674117822513, 0.48343438281031853, 0.48589857812322446, 0.4792423396467732, 0.47528670720434685, 0.47420522320685415, 0.4820408484794968, 0.470250683620504, 0.4672661452776422, 0.46446897579310625, 0.4673847070834981, 0.4667535314683005, 0.467561051568702, 0.4649346457598132, 0.454931236776774, 0.5024440397583462, 0.4513051951032547, 0.4502441373159806, 0.48183475374283263, 0.4504327395814808, 0.44976175360269477, 0.44327001845302216, 0.442759881454071, 0.4403215693650024, 0.44504379969436436, 0.4373524344368781, 0.441704384847751, 0.4358514294652173, 0.44804104836844005, 0.4325045373689771, 0.4332877341623857, 0.43271257777239186, 0.43039211909913067, 0.43396754086869743, 0.43729111473110205, 0.4471386963229371, 0.4245352911182315, 0.42215034445914884, 0.4335283508617555, 0.4289956851403873, 0.41871614183971523, 0.45475305935271004, 0.47479137679037814, 0.41566503750319544, 0.4882471680918078, 0.415616579934175, 0.4186650199065944, 0.4140433205609044, 0.42038234010861414, 0.4240694965913989, 0.4082672370312182, 0.40842830011226644, 0.4112514920971855, 0.4308078249068538, 0.4054237346492821, 0.4492079719122969, 0.43234459224708893, 0.44569085198769653, 0.4552294425822614, 0.4008497611844382, 0.523012167005965, 0.400008128092228, 0.42425204705396574, 0.39842318441221153, 0.39858988199588863, 0.427336685833453, 0.3975197023070988, 0.39413233343636006, 0.41691435222691625, 0.4162229173617541, 0.4355669985262978, 0.3939794161562867, 0.4064426592225231, 0.39857387675386063, 0.3881398230619971, 0.39175427397123713, 0.38673640214372396, 0.38618665214238396, 0.38816938541191814, 0.38707579138856324, 0.3836651704398538, 0.3835630201946036, 0.3917725652755658, 0.3875150550478987, 0.386728040952225, 0.3864741490655812, 0.39582918463982825, 0.38748144402856965, 0.37843766444870525, 0.3977754783161362, 0.3842851247062608, 0.38855089407022847, 0.37564006095844893, 0.3827835770353082]\n",
      "------------------------------------------------\n",
      "val loss:  [0.6646190952775195, 0.6515164237119583, 0.6307041686231704, 0.6290891399532217, 0.6304929937438217, 0.6208362201901493, 0.6062444170816965, 0.6532380880951432, 0.6076443315501273, 0.5952794587036999, 0.6214650604181211, 0.6058508126194757, 0.603216357346822, 0.585261201323867, 0.5887177528803663, 0.5808810542019617, 0.5821067381452095, 0.5803643727668861, 0.5792202932015043, 0.5750700778728969, 0.6067241270954076, 0.5734240064814479, 0.5731812014577775, 0.5788889593215684, 0.5702795895473923, 0.5942650144295636, 0.575586184915815, 0.5682417877849514, 0.5729535307322632, 0.5665330339293609, 0.5648816352525249, 0.565728819352655, 0.5734811611450534, 0.5637307546993011, 0.5625354452585436, 0.5612171070758918, 0.5654758990706145, 0.5660467088043534, 0.568534316494792, 0.567129510322393, 0.5581626460412947, 0.6054834471321324, 0.5571520210293404, 0.5574841105009477, 0.5881551786526648, 0.5593991499554652, 0.55982085780667, 0.5543880305121868, 0.555215543161526, 0.5542724914693433, 0.5604764955699588, 0.5538891676154909, 0.5590934597558103, 0.5533107832119936, 0.5675548942315566, 0.5529064705236083, 0.5535728455377483, 0.5551658485372193, 0.5531444665462881, 0.557626520310379, 0.5616811767825285, 0.5743757399797584, 0.5506456390639086, 0.5497432568158498, 0.56021882603543, 0.5567017969301012, 0.5488547664754305, 0.5869790558386544, 0.6078032369244792, 0.547958688839032, 0.6185083557120051, 0.5492854549661711, 0.5527013637788553, 0.5508535978179011, 0.5590693119843488, 0.5597762255537369, 0.5467737896377398, 0.5476068878707552, 0.5529385174965005, 0.5743653265481432, 0.5475260114039677, 0.5891202929617728, 0.5730648234825999, 0.5869077387766829, 0.5963772232131564, 0.5452768190984316, 0.6711398438829281, 0.547205796980679, 0.5684815026735099, 0.5470732844879539, 0.5483382021826207, 0.5732655719518658, 0.5461333889455267, 0.545293420113468, 0.5655172381851292, 0.5715590750697115, 0.592462876258665, 0.5493970986111696, 0.5641828078244441, 0.5561130960362227, 0.5433518738453489, 0.5496189274925894, 0.5430905017600167, 0.5426923052617778, 0.5445602279955098, 0.5469789338852911, 0.5427898498534715, 0.5422870021821604, 0.5545544372310017, 0.5462603520675879, 0.5464709817664191, 0.5515259389040469, 0.563102630422833, 0.5542860754539947, 0.5432375430079339, 0.5591488820772587, 0.552380688319582, 0.5579625570453565, 0.5418069212066783, 0.5469521512025083]\n"
     ]
    }
   ],
   "source": [
    "if tune_params == 'Y':\n",
    "    w_count_bocn, t_bocn, v_bocn = SGD(train_vec_bocn, train_label, X_dev=development_vec_bocn, Y_dev=development_label,\n",
    "                                      lr=lr[lr_index], alpha=alpha[alpha_index], epochs=epochs[epochs_index], print_progress=True)\n",
    "else:\n",
    "    w_count_bocn, t_bocn, v_bocn = SGD(train_vec_bocn, train_label, X_dev=development_vec_bocn, Y_dev=development_label, print_progress=True)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1520325f790>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(t_bocn, 'r')\n",
    "plt.plot(v_bocn, 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a unstable training, which means the learning rate may be too large for this model. However, the final val loss and accuracy are still acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7869674185463659\n",
      "Precision: 0.7821782178217822\n",
      "Recall: 0.7939698492462312\n",
      "F1-Score: 0.7880299251870324\n"
     ]
    }
   ],
   "source": [
    "preds_te_count_bocn = predict_class(test_vec_bocn, w_count_bocn)\n",
    "print('Accuracy:', accuracy_score(test_label,preds_te_count_bocn))\n",
    "print('Precision:', precision_score(test_label,preds_te_count_bocn))\n",
    "print('Recall:', recall_score(test_label,preds_te_count_bocn))\n",
    "print('F1-Score:', f1_score(test_label,preds_te_count_bocn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad\n",
      "rs\n",
      "ad\n",
      "te\n",
      "ors\n",
      "ba\n",
      "p\n",
      "st\n",
      "gu\n",
      "gi\n"
     ]
    }
   ],
   "source": [
    "top_neg = w_count_bocn.argsort()[:10]\n",
    "for i in top_neg:\n",
    "    print(id_to_vocab_bocn[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "li\n",
      "per\n",
      "rf\n",
      "erf\n",
      "od\n",
      "fu\n",
      "gr\n",
      "gre\n",
      "is\n",
      "la\n"
     ]
    }
   ],
   "source": [
    "top_pos = w_count_bocn.argsort()[::-1][:10]\n",
    "for i in top_pos:\n",
    "    print(id_to_vocab_bocn[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get 'bad' for the negative reviews. Also, 'per', 'gre', 'erf', seems like 'perfect' and 'great' in positive reviews. Thus, this model is also reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOCN-tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max accuracy: 0.8345864661654135\n",
      "lr:  0.0001\n",
      "alpha:  1e-06\n",
      "epochs:  165\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  lr = 1e-06  |   alpha = 1e-06    |   alpha = 1e-05    |   alpha = 0.0001   |   alpha = 0.001    |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| epochs = 30  | 0.5639097744360902 | 0.5639097744360902 | 0.5639097744360902 | 0.5639097744360902 |\n",
      "| epochs = 75  | 0.5839598997493735 | 0.5839598997493735 | 0.5839598997493735 | 0.5839598997493735 |\n",
      "| epochs = 120 | 0.6290726817042607 | 0.6290726817042607 | 0.6290726817042607 | 0.6290726817042607 |\n",
      "| epochs = 165 | 0.6466165413533834 | 0.6466165413533834 | 0.6466165413533834 | 0.6466165413533834 |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  lr = 1e-05  |   alpha = 1e-06    |   alpha = 1e-05    |   alpha = 0.0001   |   alpha = 0.001    |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| epochs = 30  | 0.6967418546365914 | 0.6967418546365914 | 0.6967418546365914 | 0.6967418546365914 |\n",
      "| epochs = 75  | 0.7619047619047619 | 0.7619047619047619 | 0.7619047619047619 | 0.7619047619047619 |\n",
      "| epochs = 120 | 0.7794486215538847 | 0.7794486215538847 | 0.7794486215538847 | 0.7794486215538847 |\n",
      "| epochs = 165 | 0.7769423558897243 | 0.7769423558897243 | 0.7769423558897243 | 0.7769423558897243 |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| lr = 0.0001  |   alpha = 1e-06    |   alpha = 1e-05    |   alpha = 0.0001   |   alpha = 0.001    |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| epochs = 30  | 0.7969924812030075 | 0.7969924812030075 | 0.7969924812030075 | 0.7969924812030075 |\n",
      "| epochs = 75  | 0.8145363408521303 | 0.8145363408521303 | 0.8145363408521303 | 0.8145363408521303 |\n",
      "| epochs = 120 | 0.8295739348370927 | 0.8295739348370927 | 0.8295739348370927 | 0.8295739348370927 |\n",
      "| epochs = 165 | 0.8345864661654135 | 0.8345864661654135 | 0.8345864661654135 | 0.8345864661654135 |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  lr = 0.001  |   alpha = 1e-06    |   alpha = 1e-05    |   alpha = 0.0001   |   alpha = 0.001    |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| epochs = 30  | 0.8320802005012531 | 0.8320802005012531 | 0.8320802005012531 | 0.8320802005012531 |\n",
      "| epochs = 75  | 0.8195488721804511 | 0.8195488721804511 | 0.8195488721804511 | 0.8195488721804511 |\n",
      "| epochs = 120 | 0.8170426065162907 | 0.8170426065162907 | 0.8170426065162907 | 0.8170426065162907 |\n",
      "| epochs = 165 | 0.8270676691729323 | 0.8270676691729323 | 0.8270676691729323 | 0.8270676691729323 |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "if tune_params == 'Y':\n",
    "    acc_history = []\n",
    "    table_lr0 = PrettyTable([\"lr = {}\".format(lr[0]), \"alpha = {}\".format(alpha[0]), \"alpha = {}\".format(alpha[1])\n",
    "                , \"alpha = {}\".format(alpha[2]), \"alpha = {}\".format(alpha[3])])\n",
    "    table_lr1 = PrettyTable([\"lr = {}\".format(lr[1]), \"alpha = {}\".format(alpha[0]), \"alpha = {}\".format(alpha[1])\n",
    "                , \"alpha = {}\".format(alpha[2]), \"alpha = {}\".format(alpha[3])])\n",
    "    table_lr2 = PrettyTable([\"lr = {}\".format(lr[2]), \"alpha = {}\".format(alpha[0]), \"alpha = {}\".format(alpha[1])\n",
    "                , \"alpha = {}\".format(alpha[2]), \"alpha = {}\".format(alpha[3])])\n",
    "    table_lr3 = PrettyTable([\"lr = {}\".format(lr[3]), \"alpha = {}\".format(alpha[0]), \"alpha = {}\".format(alpha[1])\n",
    "                , \"alpha = {}\".format(alpha[2]), \"alpha = {}\".format(alpha[3])])\n",
    "    for i in range(len(lr)):\n",
    "        for j in range(len(alpha)):\n",
    "            for k in range(len(epochs)):\n",
    "                w_tfidf_bocn, t_tfidf_bocn, v_tfidf_bocn = SGD(train_tfidf_bocn, train_label, X_dev=development_tfidf_bocn, Y_dev=development_label, lr=lr[i], alpha=alpha[j], epochs=epochs[k])\n",
    "                preds_te_count = predict_class(test_tfidf_bocn, w_tfidf_bocn)\n",
    "                if v_tfidf_bocn[len(v_tfidf_bocn)-1] < 1:\n",
    "                    acc_history.append(accuracy_score(test_label,preds_te_count))\n",
    "                else:\n",
    "                    acc_history.append(0)\n",
    "        if i == 0:\n",
    "            table_lr0.add_row([\"epochs = {}\".format(epochs[0]), acc_history[i*16+0], acc_history[i*16+4], acc_history[i*16+8], acc_history[i*16+12]])\n",
    "            table_lr0.add_row([\"epochs = {}\".format(epochs[1]), acc_history[i*16+1], acc_history[i*16+5], acc_history[i*16+9], acc_history[i*16+13]])\n",
    "            table_lr0.add_row([\"epochs = {}\".format(epochs[2]), acc_history[i*16+2], acc_history[i*16+6], acc_history[i*16+10], acc_history[i*16+14]])\n",
    "            table_lr0.add_row([\"epochs = {}\".format(epochs[3]), acc_history[i*16+3], acc_history[i*16+7], acc_history[i*16+11], acc_history[i*16+15]])\n",
    "        if i == 1:\n",
    "            table_lr1.add_row([\"epochs = {}\".format(epochs[0]), acc_history[i*16+0], acc_history[i*16+4], acc_history[i*16+8], acc_history[i*16+12]])\n",
    "            table_lr1.add_row([\"epochs = {}\".format(epochs[1]), acc_history[i*16+1], acc_history[i*16+5], acc_history[i*16+9], acc_history[i*16+13]])\n",
    "            table_lr1.add_row([\"epochs = {}\".format(epochs[2]), acc_history[i*16+2], acc_history[i*16+6], acc_history[i*16+10], acc_history[i*16+14]])\n",
    "            table_lr1.add_row([\"epochs = {}\".format(epochs[3]), acc_history[i*16+3], acc_history[i*16+7], acc_history[i*16+11], acc_history[i*16+15]])\n",
    "        if i == 2:\n",
    "            table_lr2.add_row([\"epochs = {}\".format(epochs[0]), acc_history[i*16+0], acc_history[i*16+4], acc_history[i*16+8], acc_history[i*16+12]])\n",
    "            table_lr2.add_row([\"epochs = {}\".format(epochs[1]), acc_history[i*16+1], acc_history[i*16+5], acc_history[i*16+9], acc_history[i*16+13]])\n",
    "            table_lr2.add_row([\"epochs = {}\".format(epochs[2]), acc_history[i*16+2], acc_history[i*16+6], acc_history[i*16+10], acc_history[i*16+14]])\n",
    "            table_lr2.add_row([\"epochs = {}\".format(epochs[3]), acc_history[i*16+3], acc_history[i*16+7], acc_history[i*16+11], acc_history[i*16+15]])\n",
    "        if i == 3:\n",
    "            table_lr3.add_row([\"epochs = {}\".format(epochs[0]), acc_history[i*16+0], acc_history[i*16+4], acc_history[i*16+8], acc_history[i*16+12]])\n",
    "            table_lr3.add_row([\"epochs = {}\".format(epochs[1]), acc_history[i*16+1], acc_history[i*16+5], acc_history[i*16+9], acc_history[i*16+13]])\n",
    "            table_lr3.add_row([\"epochs = {}\".format(epochs[2]), acc_history[i*16+2], acc_history[i*16+6], acc_history[i*16+10], acc_history[i*16+14]])\n",
    "            table_lr3.add_row([\"epochs = {}\".format(epochs[3]), acc_history[i*16+3], acc_history[i*16+7], acc_history[i*16+11], acc_history[i*16+15]])\n",
    "    result = max(acc_history)\n",
    "    print('max accuracy:', result)\n",
    "    index_of_result = acc_history.index(result)\n",
    "    lr_index = (index_of_result)//16\n",
    "    alpha_index = (index_of_result%16)//4\n",
    "    epochs_index = (index_of_result%4)\n",
    "    print('lr: ', lr[lr_index])\n",
    "    print('alpha: ', alpha[alpha_index])\n",
    "    print('epochs: ', epochs[epochs_index])\n",
    "    print(table_lr0)\n",
    "    print(table_lr1)\n",
    "    print(table_lr2)\n",
    "    print(table_lr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  [0.681458442528966, 0.6711049079825542, 0.6615120432983105, 0.6525627309449156, 0.644122907984897, 0.6361412955962436, 0.628572718914092, 0.6213843236872093, 0.6145407854238141, 0.6080267079756276, 0.6018109413717951, 0.5958745143890353, 0.5901966065070277, 0.5847585483407477, 0.5795432190294567, 0.5745344793852059, 0.5697189752673516, 0.565080162874092, 0.5606187557619792, 0.5563127654729154, 0.5521559545579795, 0.5481459773742027, 0.5442693642569337, 0.5405060459593192, 0.536866882725984, 0.5333422719183369, 0.529922478049714, 0.5266035268968914, 0.5233796612711548, 0.520242842405782, 0.5171968429622555, 0.5142292337685429, 0.511340007973965, 0.5085263546740816, 0.5057828769010678, 0.5031063165920482, 0.5004938093366652, 0.49794630533921563, 0.49545441537364154, 0.49302266495941144, 0.4906438404704505, 0.4883188390709106, 0.48604404691858927, 0.4838176166001264, 0.4816382148925806, 0.4795036303255345, 0.4774131552726171, 0.47536227280423987, 0.47335248372816224, 0.4713823178914615, 0.46944905402040366, 0.46755205507576697, 0.4656904636909933, 0.46386093124806826, 0.46206556787016134, 0.4603020821597286, 0.4585683813426348, 0.45686612889525485, 0.45519037012553737, 0.4535436447033751, 0.4519233514766584, 0.45032983360154566, 0.4487620742249391, 0.4472185157354583, 0.44569938695979, 0.4442039595563238, 0.4427313739348888, 0.4412817323570726, 0.4398547969818796, 0.4384443659759084, 0.43705789324511063, 0.43568969054389395, 0.4343406451558378, 0.4330113544204128, 0.4317012271780796, 0.43040746331129986, 0.42913215384112785, 0.42787347806891274, 0.42663184706678897, 0.42540665301000197, 0.4241974885355095, 0.4230034818452375, 0.4218249995380631, 0.4206611680598982, 0.41951192346831434, 0.4183774283275817, 0.41725548066532736, 0.41614705631896093, 0.41505260515194037, 0.41397107167391967, 0.41290220911176456, 0.41184532914874816, 0.410800563860188, 0.40976822991741657, 0.4087474451731889, 0.4077377787990035, 0.40673954357197667, 0.40575214917273994, 0.4047762866235605, 0.4038098039326749, 0.40285409552400153, 0.40190851308442477, 0.40097290654278595, 0.4000470420812517, 0.3991308510580404, 0.39822424970235565, 0.39732645917357484, 0.39643813771853165, 0.39555824062776607, 0.39468711835766285, 0.39382493714226907, 0.3929709331585915, 0.39212509736093937, 0.3912877235890065, 0.3904577642661508, 0.38963598928256304, 0.3888218881635632, 0.3880153516317313, 0.38721634663881627, 0.3864245605211805, 0.3856400489538874, 0.3848627782031098, 0.3840921972391298, 0.38332839915448, 0.38257170556022435, 0.3818213429678281, 0.3810776531376982, 0.3803402298463152, 0.3796092159310637, 0.37888414007629867, 0.37816550047926056, 0.37745303985274864, 0.37674628154433365, 0.37604543659643147, 0.3753503001473401, 0.3746610223936662, 0.37397718044086264, 0.3732988710326554, 0.37262634468975486, 0.37195849276196535, 0.37129645505368986, 0.37063909331930034, 0.3699871900393843, 0.36934039307421657, 0.3686985165117873, 0.36806174375174505, 0.36742968004665083, 0.36680237768847374, 0.36617986723157886, 0.3655619026241726, 0.36494855642306234, 0.3643396608462779, 0.36373507088954443, 0.3631349933908765, 0.36253943638312935, 0.3619482079772649, 0.3613608998697336, 0.36077790821619365, 0.3601990116875522, 0.35962466052003095, 0.359053430026595, 0.358486647475162, 0.35792376429268796, 0.35736450114704477, 0.3568093340000903]\n",
      "------------------------------------------------\n",
      "val loss:  [0.6834083006124824, 0.6749253367257749, 0.6671082381026187, 0.6598886073269211, 0.6531256342898885, 0.6467613922060683, 0.6407537677160425, 0.6350796513195082, 0.6297252824367883, 0.6246415755543845, 0.6198212468768168, 0.6152519484926106, 0.6109068374732151, 0.606769163506183, 0.6028226455488358, 0.5990621161233635, 0.5954725839086251, 0.5920423302567313, 0.5887641594929917, 0.5856235145600309, 0.5826144367095336, 0.5797337670445599, 0.5769702187859095, 0.5743058831012909, 0.5717468475493378, 0.5692886489693283, 0.5669232948019062, 0.564648146696283, 0.5624519070925708, 0.5603219185436017, 0.5582736936873525, 0.5562944039229171, 0.5543912888424071, 0.5525356710585876, 0.5507455977608238, 0.5490151090362343, 0.547338130092621, 0.5457111633585945, 0.544142263380822, 0.542614726742417, 0.5411345578357055, 0.5397000740791217, 0.538301690284257, 0.5369402535976249, 0.5356184903231817, 0.5343382479616104, 0.5331034181981584, 0.5318835355794476, 0.5307049658824505, 0.5295460575735462, 0.5284364591313824, 0.527350841440798, 0.526295110911051, 0.5252347666088922, 0.5242354301408176, 0.5232557342265556, 0.522277476503812, 0.5213571655878169, 0.5204294942183693, 0.5195351303395905, 0.5186487580699454, 0.5177851928071755, 0.5169441390231758, 0.5161348002812944, 0.5153366917407438, 0.5145590679931507, 0.5138069517423969, 0.513070454989694, 0.5123554910549195, 0.5116172100288376, 0.5109313915382705, 0.5102051164043659, 0.5095520615864637, 0.5089021281744496, 0.5082683440929373, 0.50762855533666, 0.5069944024424939, 0.506391761467474, 0.5058064127878441, 0.5052124375928554, 0.5046400426998101, 0.5040870476421235, 0.5035399356042609, 0.5030072889009319, 0.502483878030043, 0.5019802342616816, 0.501464434047776, 0.5009501489959505, 0.500451635728508, 0.49996624629507763, 0.49952470017075196, 0.4990386457004219, 0.49858905457417246, 0.49814335382703256, 0.49769025013979756, 0.49727344042757937, 0.49685940340899454, 0.49643435307668515, 0.49604597318790855, 0.4956272518484385, 0.4952310552343427, 0.4948348898347804, 0.49446689544380934, 0.4940835542476692, 0.49371231465619675, 0.493341863478258, 0.4929927532846747, 0.49262676856791626, 0.49230393362987135, 0.4919664380125923, 0.4916362400720357, 0.49130433530895623, 0.4909830844912481, 0.4906828736985571, 0.4903588146016524, 0.49004258735734163, 0.4897473433229131, 0.48944600891885376, 0.4891461701309081, 0.4888606870027838, 0.4885927868264016, 0.4883143722679472, 0.48804309471357504, 0.48776159722220563, 0.48748086004791796, 0.48722937914607634, 0.48696240503368515, 0.48672476405919074, 0.4864562399222419, 0.48622200265885607, 0.4859799665972549, 0.48575166410492293, 0.4855107743577757, 0.4852695736559774, 0.4850457457621577, 0.4848134166152787, 0.48460901571695836, 0.48438587463237454, 0.484188533265841, 0.48396169811445766, 0.4837728067992811, 0.483542050324524, 0.48333955152478425, 0.4831417835478862, 0.48294110225371767, 0.48274009898553005, 0.4825548166746792, 0.48237473535466246, 0.4821893645886932, 0.48201657447341206, 0.48184143454580197, 0.48166230302597834, 0.4814773883390656, 0.4813199553035722, 0.4811599299338769, 0.4809929637288825, 0.48082184126599675, 0.48065125476693527, 0.48048696214816866, 0.4803101774199123, 0.4801739858066424, 0.4800121933321794, 0.4798638975642629, 0.4797356991470545, 0.47957865383553094]\n"
     ]
    }
   ],
   "source": [
    "if tune_params == 'Y':\n",
    "    w_tfidf_bocn, t_tfidf_bocn, v_tfidf_bocn = SGD(train_tfidf_bocn, train_label, X_dev=development_tfidf_bocn,\n",
    "                                                   Y_dev=development_label,lr=lr[lr_index], alpha=alpha[alpha_index],\n",
    "                                                   epochs=epochs[epochs_index], print_progress=True)\n",
    "else:\n",
    "    w_tfidf_bocn, t_tfidf_bocn, v_tfidf_bocn = SGD(train_tfidf_bocn, train_label,\n",
    "                                                   X_dev=development_tfidf_bocn, Y_dev=development_label,print_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x152032db0a0>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(t_tfidf_bocn, 'r')\n",
    "plt.plot(v_tfidf_bocn, 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final val loss is about 0.5, which is acceptable.\n",
    "The curve of val loss is descending without a trend of ascending. This means the model is neither overfitting nor underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8345864661654135\n",
      "Precision: 0.8181818181818182\n",
      "Recall: 0.8592964824120602\n",
      "F1-Score: 0.838235294117647\n"
     ]
    }
   ],
   "source": [
    "preds_te_tfidf_bocn = predict_class(test_tfidf_bocn, w_tfidf_bocn)\n",
    "print('Accuracy:', accuracy_score(test_label,preds_te_tfidf_bocn))\n",
    "print('Precision:', precision_score(test_label,preds_te_tfidf_bocn))\n",
    "print('Recall:', recall_score(test_label,preds_te_tfidf_bocn))\n",
    "print('F1-Score:', f1_score(test_label,preds_te_tfidf_bocn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad\n",
      "bor\n",
      "ipt\n",
      "ulo\n",
      "wfu\n",
      "ors\n",
      "awf\n",
      "mpt\n",
      "wf\n",
      "upi\n"
     ]
    }
   ],
   "source": [
    "top_neg = w_tfidf_bocn.argsort()[:10]\n",
    "for i in top_neg:\n",
    "    print(id_to_vocab_bocn[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rfe\n",
      "ila\n",
      "erf\n",
      "gre\n",
      "rf\n",
      "rue\n",
      "rld\n",
      "osc\n",
      "rfu\n",
      "fic\n"
     ]
    }
   ],
   "source": [
    "top_pos = w_tfidf_bocn.argsort()[::-1][:10]\n",
    "for i in top_pos:\n",
    "    print(id_to_vocab_bocn[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get 'bad', 'awf', 'wuf', 'wf' for 'bad' and 'awful' in negative reviews. Similarly, we get 'rfe', 'erf', and 'rf' for 'perfect' in positive reviews. Thus, this model is also reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW+BOCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bow_bocn_tfidf = np.concatenate((train_tfidf,train_tfidf_bocn), axis=1)\n",
    "development_bow_bocn_tfidf = np.concatenate((development_tfidf,development_tfidf_bocn), axis=1)\n",
    "test_bow_bocn_tfidf = np.concatenate((test_tfidf,test_tfidf_bocn), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max accuracy: 0.8571428571428571\n",
      "lr:  0.001\n",
      "alpha:  1e-06\n",
      "epochs:  30\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  lr = 1e-06  |   alpha = 1e-06    |   alpha = 1e-05    |   alpha = 0.0001   |   alpha = 0.001    |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| epochs = 30  | 0.6165413533834586 | 0.6165413533834586 | 0.6165413533834586 | 0.6165413533834586 |\n",
      "| epochs = 75  | 0.6466165413533834 | 0.6466165413533834 | 0.6466165413533834 | 0.6466165413533834 |\n",
      "| epochs = 120 | 0.6917293233082706 | 0.6917293233082706 | 0.6917293233082706 | 0.6917293233082706 |\n",
      "| epochs = 165 | 0.7192982456140351 | 0.7192982456140351 | 0.7192982456140351 | 0.7192982456140351 |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  lr = 1e-05  |   alpha = 1e-06    |   alpha = 1e-05    |   alpha = 0.0001   |   alpha = 0.001    |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| epochs = 30  | 0.7543859649122807 | 0.7543859649122807 | 0.7543859649122807 | 0.7543859649122807 |\n",
      "| epochs = 75  | 0.7944862155388471 | 0.7944862155388471 | 0.7944862155388471 | 0.7944862155388471 |\n",
      "| epochs = 120 | 0.8020050125313283 | 0.8020050125313283 | 0.8020050125313283 | 0.8020050125313283 |\n",
      "| epochs = 165 | 0.8120300751879699 | 0.8120300751879699 | 0.8120300751879699 | 0.8120300751879699 |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| lr = 0.0001  |   alpha = 1e-06    |   alpha = 1e-05    |   alpha = 0.0001   |   alpha = 0.001    |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| epochs = 30  | 0.8370927318295739 | 0.8370927318295739 | 0.8370927318295739 | 0.8370927318295739 |\n",
      "| epochs = 75  | 0.8421052631578947 | 0.8421052631578947 | 0.8421052631578947 | 0.8421052631578947 |\n",
      "| epochs = 120 | 0.849624060150376  | 0.849624060150376  | 0.849624060150376  | 0.849624060150376  |\n",
      "| epochs = 165 | 0.849624060150376  | 0.849624060150376  | 0.849624060150376  | 0.849624060150376  |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  lr = 0.001  |   alpha = 1e-06    |   alpha = 1e-05    |   alpha = 0.0001   |   alpha = 0.001    |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| epochs = 30  | 0.8571428571428571 | 0.8571428571428571 | 0.8571428571428571 | 0.8571428571428571 |\n",
      "| epochs = 75  | 0.8471177944862155 | 0.8471177944862155 | 0.8471177944862155 | 0.8471177944862155 |\n",
      "| epochs = 120 | 0.8421052631578947 | 0.8421052631578947 | 0.8421052631578947 | 0.8421052631578947 |\n",
      "| epochs = 165 | 0.8345864661654135 | 0.8345864661654135 | 0.8345864661654135 | 0.8345864661654135 |\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "if tune_params == 'Y':\n",
    "    acc_history = []\n",
    "    table_lr0 = PrettyTable([\"lr = {}\".format(lr[0]), \"alpha = {}\".format(alpha[0]), \"alpha = {}\".format(alpha[1])\n",
    "                , \"alpha = {}\".format(alpha[2]), \"alpha = {}\".format(alpha[3])])\n",
    "    table_lr1 = PrettyTable([\"lr = {}\".format(lr[1]), \"alpha = {}\".format(alpha[0]), \"alpha = {}\".format(alpha[1])\n",
    "                , \"alpha = {}\".format(alpha[2]), \"alpha = {}\".format(alpha[3])])\n",
    "    table_lr2 = PrettyTable([\"lr = {}\".format(lr[2]), \"alpha = {}\".format(alpha[0]), \"alpha = {}\".format(alpha[1])\n",
    "                , \"alpha = {}\".format(alpha[2]), \"alpha = {}\".format(alpha[3])])\n",
    "    table_lr3 = PrettyTable([\"lr = {}\".format(lr[3]), \"alpha = {}\".format(alpha[0]), \"alpha = {}\".format(alpha[1])\n",
    "                , \"alpha = {}\".format(alpha[2]), \"alpha = {}\".format(alpha[3])])\n",
    "    for i in range(len(lr)):\n",
    "        for j in range(len(alpha)):\n",
    "            for k in range(len(epochs)):\n",
    "                w_bow_bocn_tfidf, t_bow_bocn_tfidf, v_bow_bocn_tfidf = SGD(X_tr=train_bow_bocn_tfidf, Y_tr=train_label, X_dev=development_bow_bocn_tfidf, Y_dev=development_label, lr=lr[i], alpha=alpha[j], epochs=epochs[k])\n",
    "                preds_te_bow_bocn_tfidf = predict_class(test_bow_bocn_tfidf, w_bow_bocn_tfidf)\n",
    "                if v_bow_bocn_tfidf[len(v_bow_bocn_tfidf)-1] < 1:\n",
    "                    acc_history.append(accuracy_score(test_label,preds_te_bow_bocn_tfidf))\n",
    "                else:\n",
    "                    acc_history.append(0)\n",
    "        if i == 0:\n",
    "            table_lr0.add_row([\"epochs = {}\".format(epochs[0]), acc_history[i*16+0], acc_history[i*16+4], acc_history[i*16+8], acc_history[i*16+12]])\n",
    "            table_lr0.add_row([\"epochs = {}\".format(epochs[1]), acc_history[i*16+1], acc_history[i*16+5], acc_history[i*16+9], acc_history[i*16+13]])\n",
    "            table_lr0.add_row([\"epochs = {}\".format(epochs[2]), acc_history[i*16+2], acc_history[i*16+6], acc_history[i*16+10], acc_history[i*16+14]])\n",
    "            table_lr0.add_row([\"epochs = {}\".format(epochs[3]), acc_history[i*16+3], acc_history[i*16+7], acc_history[i*16+11], acc_history[i*16+15]])\n",
    "        if i == 1:\n",
    "            table_lr1.add_row([\"epochs = {}\".format(epochs[0]), acc_history[i*16+0], acc_history[i*16+4], acc_history[i*16+8], acc_history[i*16+12]])\n",
    "            table_lr1.add_row([\"epochs = {}\".format(epochs[1]), acc_history[i*16+1], acc_history[i*16+5], acc_history[i*16+9], acc_history[i*16+13]])\n",
    "            table_lr1.add_row([\"epochs = {}\".format(epochs[2]), acc_history[i*16+2], acc_history[i*16+6], acc_history[i*16+10], acc_history[i*16+14]])\n",
    "            table_lr1.add_row([\"epochs = {}\".format(epochs[3]), acc_history[i*16+3], acc_history[i*16+7], acc_history[i*16+11], acc_history[i*16+15]])\n",
    "        if i == 2:\n",
    "            table_lr2.add_row([\"epochs = {}\".format(epochs[0]), acc_history[i*16+0], acc_history[i*16+4], acc_history[i*16+8], acc_history[i*16+12]])\n",
    "            table_lr2.add_row([\"epochs = {}\".format(epochs[1]), acc_history[i*16+1], acc_history[i*16+5], acc_history[i*16+9], acc_history[i*16+13]])\n",
    "            table_lr2.add_row([\"epochs = {}\".format(epochs[2]), acc_history[i*16+2], acc_history[i*16+6], acc_history[i*16+10], acc_history[i*16+14]])\n",
    "            table_lr2.add_row([\"epochs = {}\".format(epochs[3]), acc_history[i*16+3], acc_history[i*16+7], acc_history[i*16+11], acc_history[i*16+15]])\n",
    "        if i == 3:\n",
    "            table_lr3.add_row([\"epochs = {}\".format(epochs[0]), acc_history[i*16+0], acc_history[i*16+4], acc_history[i*16+8], acc_history[i*16+12]])\n",
    "            table_lr3.add_row([\"epochs = {}\".format(epochs[1]), acc_history[i*16+1], acc_history[i*16+5], acc_history[i*16+9], acc_history[i*16+13]])\n",
    "            table_lr3.add_row([\"epochs = {}\".format(epochs[2]), acc_history[i*16+2], acc_history[i*16+6], acc_history[i*16+10], acc_history[i*16+14]])\n",
    "            table_lr3.add_row([\"epochs = {}\".format(epochs[3]), acc_history[i*16+3], acc_history[i*16+7], acc_history[i*16+11], acc_history[i*16+15]])\n",
    "    result = max(acc_history)\n",
    "    print('max accuracy:', result)\n",
    "    index_of_result = acc_history.index(result)\n",
    "    lr_index = (index_of_result)//16\n",
    "    alpha_index = (index_of_result%16)//4\n",
    "    epochs_index = (index_of_result%4)\n",
    "    print('lr: ', lr[lr_index])\n",
    "    print('alpha: ', alpha[alpha_index])\n",
    "    print('epochs: ', epochs[epochs_index])\n",
    "    print(table_lr0)\n",
    "    print(table_lr1)\n",
    "    print(table_lr2)\n",
    "    print(table_lr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  [0.5326226599730167, 0.4608078445091674, 0.4171088783073348, 0.38533661616981485, 0.3612415661834907, 0.34203780290868385, 0.3261362514754283, 0.3125426275771912, 0.30092211849012984, 0.29043550946735486, 0.281296956930362, 0.2728883377198892, 0.2653410593908214, 0.258502002935498, 0.25214064668513586, 0.24680485572422978, 0.2408931465385257, 0.23602232448544883, 0.23117783271573916, 0.22651852156520985, 0.22259348905669524, 0.2193859255885909, 0.2149572798015082, 0.21098378943928875, 0.20755885359677753, 0.20434022663679863, 0.20126684908663325, 0.198481190758188, 0.19540744074028224, 0.1931055893962769]\n",
      "------------------------------------------------\n",
      "val loss:  [0.5802836441951238, 0.5364640899039577, 0.5133538564140353, 0.4975822807445059, 0.48759905759390165, 0.4804719340733244, 0.47529928541722805, 0.4710506616489357, 0.46816921363955033, 0.4673238564444099, 0.4660015386253393, 0.4644367226877555, 0.46354491325260716, 0.46301853090826534, 0.463049482937284, 0.4648087464532036, 0.4639172046919725, 0.4636600988866982, 0.4658726365463701, 0.46579918730765457, 0.4676984482008383, 0.4702113085165863, 0.47007049044599875, 0.47022673722423824, 0.4711932045635471, 0.47232553415385947, 0.4737444072439565, 0.4763071403940996, 0.4768602909254012, 0.4774864400802157]\n"
     ]
    }
   ],
   "source": [
    "if tune_params == 'Y':\n",
    "    w_bow_bocn_tfidf, t_bow_bocn_tfidf, v_bow_bocn_tfidf = SGD(train_bow_bocn_tfidf, train_label, X_dev=development_bow_bocn_tfidf,\n",
    "                                                               Y_dev=development_label, lr=lr[lr_index], alpha=alpha[alpha_index],\n",
    "                                                               epochs=epochs[epochs_index], print_progress=True)\n",
    "else:\n",
    "    w_bow_bocn_tfidf, t_bow_bocn_tfidf, v_bow_bocn_tfidf = SGD(train_bow_bocn_tfidf, train_label, X_dev=development_bow_bocn_tfidf,\n",
    "                                                               Y_dev=development_label, print_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x152033f1490>]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(t_bow_bocn_tfidf, 'r')\n",
    "plt.plot(v_bow_bocn_tfidf, 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final val loss is about 0.5, which is acceptable.\n",
    "The curve of val loss is descending without a trend of ascending. This means the model is neither overfitting nor underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8571428571428571\n",
      "Precision: 0.855\n",
      "Recall: 0.8592964824120602\n",
      "F1-Score: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "preds_te_bow_bocn_tfidf = predict_class(test_bow_bocn_tfidf, w_bow_bocn_tfidf)\n",
    "print('Accuracy:', accuracy_score(test_label,preds_te_bow_bocn_tfidf))\n",
    "print('Precision:', precision_score(test_label,preds_te_bow_bocn_tfidf))\n",
    "print('Recall:', recall_score(test_label,preds_te_bow_bocn_tfidf))\n",
    "print('F1-Score:', f1_score(test_label,preds_te_bow_bocn_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_vocab_bow_bocn = {**id_to_vocab_bocn, **id_to_vocab}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad\n",
      "cam\n",
      "waste\n",
      "worst\n",
      "boring\n",
      "supposed\n",
      "fails\n",
      "flat\n",
      "worse\n",
      "ridiculous\n"
     ]
    }
   ],
   "source": [
    "top_neg = w_bow_bocn_tfidf.argsort()[:10]\n",
    "for i in top_neg:\n",
    "    print(id_to_vocab_bow_bocn[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hilarious\n",
      "great\n",
      "excellent\n",
      "simple\n",
      "terrific\n",
      "perfect\n",
      "fiction\n",
      "nfe\n",
      "memorable\n",
      "perfectly\n"
     ]
    }
   ],
   "source": [
    "top_pos = w_bow_bocn_tfidf.argsort()[::-1][:10]\n",
    "for i in top_pos:\n",
    "    print(id_to_vocab_bow_bocn[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get 'fails', 'worse', etc. for negative reviews and 'great', 'terrific', etc. for positive reviews. Thus, this model performs well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Full Results\n",
    "\n",
    "Add here your results:\n",
    "\n",
    "| LR | Precision  | Recall  | F1-Score  |\n",
    "|:-:|:-:|:-:|:-:|\n",
    "| BOW-count  |0.8177339901477833|0.8341708542713567|0.8258706467661691|\n",
    "| BOW-tfidf  |0.8333333333333334|0.8542713567839196|0.8436724565756824|\n",
    "| BOCN-count  |0.7821782178217822|0.7939698492462312|0.7880299251870324|\n",
    "| BOCN-tfidf  |0.8181818181818182|0.8592964824120602|0.838235294117647|\n",
    "| BOW+BOCN  |0.855|0.8592964824120602|0.8571428571428571|\n",
    "\n",
    "Please discuss why your best performing model is better than the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANS:\n",
    "My best performing model is BOW+BOCN. This is because this model has more features than the other four models. Also, this model does not include many noises as features. Thus, this model performs best."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
